{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Hypothesis: Style transfer using t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c360625890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.models.predict import detoxificate_style_transfer, PREFIX\n",
    "from src.models.train import train\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading t5-small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": toxic_sentences, \"labels\": non_toxic_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d707ac095884a0a85ce06a992f4b2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [PREFIX + ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Hugging Face Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57b87e589f04fc19298f039f2402767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8124, 'learning_rate': 1.9633498259116732e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2114, 'learning_rate': 1.9266996518233462e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1863, 'learning_rate': 1.8900494777350193e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1781, 'learning_rate': 1.8533993036466923e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1705, 'learning_rate': 1.8167491295583654e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1667, 'learning_rate': 1.7800989554700384e-05, 'epoch': 0.55}\n",
      "{'loss': 0.168, 'learning_rate': 1.7434487813817118e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1655, 'learning_rate': 1.7067986072933848e-05, 'epoch': 0.73}\n",
      "{'loss': 0.1668, 'learning_rate': 1.670148433205058e-05, 'epoch': 0.82}\n",
      "{'loss': 0.1589, 'learning_rate': 1.633498259116731e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646cf26be8ea4fc1bf27602f1975eeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1460123360157013, 'eval_runtime': 79.3667, 'eval_samples_per_second': 122.23, 'eval_steps_per_second': 7.648, 'epoch': 1.0}\n",
      "{'loss': 0.1607, 'learning_rate': 1.596848085028404e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1586, 'learning_rate': 1.560197910940077e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1564, 'learning_rate': 1.5235477368517502e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1561, 'learning_rate': 1.4868975627634232e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1577, 'learning_rate': 1.4502473886750962e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1562, 'learning_rate': 1.4135972145867693e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1534, 'learning_rate': 1.3769470404984425e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1532, 'learning_rate': 1.3402968664101155e-05, 'epoch': 1.65}\n",
      "{'loss': 0.1563, 'learning_rate': 1.3036466923217886e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1518, 'learning_rate': 1.2669965182334618e-05, 'epoch': 1.83}\n",
      "{'loss': 0.152, 'learning_rate': 1.2303463441451348e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc44389ba02241e9806f0d7070f4eb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13942165672779083, 'eval_runtime': 79.1137, 'eval_samples_per_second': 122.621, 'eval_steps_per_second': 7.673, 'epoch': 2.0}\n",
      "{'loss': 0.1525, 'learning_rate': 1.1936961700568079e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1502, 'learning_rate': 1.157045995968481e-05, 'epoch': 2.11}\n",
      "{'loss': 0.1491, 'learning_rate': 1.1203958218801541e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1486, 'learning_rate': 1.0837456477918271e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1488, 'learning_rate': 1.0470954737035002e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1503, 'learning_rate': 1.0104452996151732e-05, 'epoch': 2.47}\n",
      "{'loss': 0.1471, 'learning_rate': 9.737951255268463e-06, 'epoch': 2.57}\n",
      "{'loss': 0.1476, 'learning_rate': 9.371449514385193e-06, 'epoch': 2.66}\n",
      "{'loss': 0.15, 'learning_rate': 9.004947773501925e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1525, 'learning_rate': 8.638446032618655e-06, 'epoch': 2.84}\n",
      "{'loss': 0.1494, 'learning_rate': 8.271944291735386e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b2f35ec72945f293e6d11e00415b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1365615874528885, 'eval_runtime': 78.949, 'eval_samples_per_second': 122.877, 'eval_steps_per_second': 7.689, 'epoch': 3.0}\n",
      "{'loss': 0.1476, 'learning_rate': 7.905442550852118e-06, 'epoch': 3.02}\n",
      "{'loss': 0.1507, 'learning_rate': 7.538940809968847e-06, 'epoch': 3.12}\n",
      "{'loss': 0.1468, 'learning_rate': 7.172439069085579e-06, 'epoch': 3.21}\n",
      "{'loss': 0.147, 'learning_rate': 6.80593732820231e-06, 'epoch': 3.3}\n",
      "{'loss': 0.1454, 'learning_rate': 6.43943558731904e-06, 'epoch': 3.39}\n",
      "{'loss': 0.1478, 'learning_rate': 6.0729338464357714e-06, 'epoch': 3.48}\n",
      "{'loss': 0.1463, 'learning_rate': 5.706432105552502e-06, 'epoch': 3.57}\n",
      "{'loss': 0.1456, 'learning_rate': 5.339930364669232e-06, 'epoch': 3.67}\n",
      "{'loss': 0.1486, 'learning_rate': 4.973428623785963e-06, 'epoch': 3.76}\n",
      "{'loss': 0.1465, 'learning_rate': 4.606926882902694e-06, 'epoch': 3.85}\n",
      "{'loss': 0.1457, 'learning_rate': 4.240425142019425e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312d9be62bee4cbf926b183db913b877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1352386772632599, 'eval_runtime': 79.053, 'eval_samples_per_second': 122.715, 'eval_steps_per_second': 7.678, 'epoch': 4.0}\n",
      "{'loss': 0.1427, 'learning_rate': 3.873923401136156e-06, 'epoch': 4.03}\n",
      "{'loss': 0.1461, 'learning_rate': 3.5074216602528866e-06, 'epoch': 4.12}\n",
      "{'loss': 0.1455, 'learning_rate': 3.140919919369617e-06, 'epoch': 4.21}\n",
      "{'loss': 0.1456, 'learning_rate': 2.7744181784863483e-06, 'epoch': 4.31}\n",
      "{'loss': 0.1442, 'learning_rate': 2.407916437603079e-06, 'epoch': 4.4}\n",
      "{'loss': 0.1465, 'learning_rate': 2.0414146967198094e-06, 'epoch': 4.49}\n",
      "{'loss': 0.1491, 'learning_rate': 1.6749129558365405e-06, 'epoch': 4.58}\n",
      "{'loss': 0.1438, 'learning_rate': 1.308411214953271e-06, 'epoch': 4.67}\n",
      "{'loss': 0.1432, 'learning_rate': 9.419094740700019e-07, 'epoch': 4.76}\n",
      "{'loss': 0.1454, 'learning_rate': 5.754077331867327e-07, 'epoch': 4.86}\n",
      "{'loss': 0.1455, 'learning_rate': 2.0890599230346348e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273d07ecd00b4af5814753d5767981dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1347780078649521, 'eval_runtime': 78.979, 'eval_samples_per_second': 122.83, 'eval_steps_per_second': 7.686, 'epoch': 5.0}\n",
      "{'train_runtime': 10779.71, 'train_samples_per_second': 40.495, 'train_steps_per_second': 2.531, 'train_loss': 0.16572755782491058, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train('seq2seq', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=5,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hurt you and I want to find out who that was.\n",
      "Detoxified: you and I want to find out who that was.\n",
      "\n",
      "Original: I can't believe we haven't fucked for two years, nine months, three weeks and... 69 hours.\n",
      "Detoxified: I can't believe we haven't slept for two years, nine months, three weeks and... 69 hours.\n",
      "\n",
      "Original: So forgive me for being a little fidgety, but if it's lunch we're talking, I'm gonna eat a fat pork sandwich, and I'm sure as shit not gonna eat it here.\n",
      "Detoxified: so forgive me for being a little bit of a snitch, but if it's lunch we're talking, I'll eat a fat pork sandwich, and I'm sure I'm not gonna eat it here.\n",
      "\n",
      "Original: There is no fucking soy milk!\n",
      "Detoxified: there's no soy milk!\n",
      "\n",
      "Original: What's wrong with people having sex?\n",
      "Detoxified: what's wrong with people having sex?\n",
      "\n",
      "Original: What the fuck are you talking about?\n",
      "Detoxified: what are you talking about?\n",
      "\n",
      "Original: There were fewer horses that needed stealing than men who needed killing, after all.\n",
      "Detoxified: there were fewer horses that needed to steal than men who needed to kill.\n",
      "\n",
      "Original: Hey, Bob, thanks for giving us a chance to grieve together. Shit!\n",
      "Detoxified: hey, Bob, thanks for giving us a chance to grieve together.\n",
      "\n",
      "Original: But my landlord's another fucking story-- Trying to kick me out of my home.\n",
      "Detoxified: but my landlord is another story - trying to get me out of my home.\n",
      "\n",
      "Original: Gryilus, the father of young Phylon. I slashed his face with a whip.\n",
      "Detoxified: Gryilus, the father of Phylon, I slashed his face with a whip.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = AutoModelForSeq2SeqLM.from_pretrained(\"../models/detoxificator\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/detoxificator\")\n",
    "\n",
    "test_toxic_sentences = pd.read_csv('../data/interim/test.csv')['reference'].to_list()[:10]\n",
    "\n",
    "detoxified = detoxificate_style_transfer(test_toxic_sentences, best_model, tokenizer)\n",
    "\n",
    "for sentence, detoxified_sentence in zip(test_toxic_sentences, detoxified):\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Detoxified: {detoxified_sentence}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Hypothesis: Style transfer using t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f475495890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.models.predict import detoxificate_style_transfer, PREFIX\n",
    "from src.models.train import train\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading t5-small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": toxic_sentences, \"labels\": non_toxic_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819999e24ac44528ad780f666c806df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [PREFIX + ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Hugging Face Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9af840aeac4e569a2f074b47b28f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9131, 'learning_rate': 1.8249299719887958e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552f2f29683348929a0912e2a462228c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3491753041744232, 'eval_runtime': 82.8968, 'eval_samples_per_second': 122.49, 'eval_steps_per_second': 7.66, 'epoch': 0.09}\n",
      "{'loss': 0.3268, 'learning_rate': 1.649859943977591e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec899362d8942928d5725b42de27463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25028446316719055, 'eval_runtime': 83.8954, 'eval_samples_per_second': 121.032, 'eval_steps_per_second': 7.569, 'epoch': 0.18}\n",
      "{'loss': 0.2768, 'learning_rate': 1.4747899159663868e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d45648e11ef4ba0b9ce30b06f014df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24375779926776886, 'eval_runtime': 84.3467, 'eval_samples_per_second': 120.384, 'eval_steps_per_second': 7.528, 'epoch': 0.26}\n",
      "{'loss': 0.2687, 'learning_rate': 1.2997198879551822e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9514bd75a0ad4840b6e8ed1752d8d766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24030275642871857, 'eval_runtime': 84.4608, 'eval_samples_per_second': 120.222, 'eval_steps_per_second': 7.518, 'epoch': 0.35}\n",
      "{'loss': 0.2645, 'learning_rate': 1.1246498599439776e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ca326d0646406894ce63fc782212aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2379130870103836, 'eval_runtime': 83.3083, 'eval_samples_per_second': 121.885, 'eval_steps_per_second': 7.622, 'epoch': 0.44}\n",
      "{'loss': 0.2608, 'learning_rate': 9.49579831932773e-06, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cc0701cfb64dd18f35e0cc2bb85035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2362850159406662, 'eval_runtime': 83.0681, 'eval_samples_per_second': 122.237, 'eval_steps_per_second': 7.644, 'epoch': 0.53}\n",
      "{'loss': 0.2558, 'learning_rate': 7.745098039215687e-06, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a9ebeea03d4916b34ec78e616f6301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23515287041664124, 'eval_runtime': 82.9637, 'eval_samples_per_second': 122.391, 'eval_steps_per_second': 7.654, 'epoch': 0.61}\n",
      "{'loss': 0.2593, 'learning_rate': 5.994397759103642e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22b94f560f343909d8d2291d3174ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23446246981620789, 'eval_runtime': 82.9348, 'eval_samples_per_second': 122.434, 'eval_steps_per_second': 7.657, 'epoch': 0.7}\n",
      "{'loss': 0.2543, 'learning_rate': 4.243697478991597e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ab68e07ab648fabfbc142ef1f133c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23362945020198822, 'eval_runtime': 83.021, 'eval_samples_per_second': 122.306, 'eval_steps_per_second': 7.649, 'epoch': 0.79}\n",
      "{'loss': 0.2559, 'learning_rate': 2.492997198879552e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817c1cebfd440269d826170306ea2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23317529261112213, 'eval_runtime': 83.0078, 'eval_samples_per_second': 122.326, 'eval_steps_per_second': 7.65, 'epoch': 0.88}\n",
      "{'loss': 0.2549, 'learning_rate': 7.42296918767507e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdfb0ab502f4d649936ac52ef4362ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23297300934791565, 'eval_runtime': 82.9367, 'eval_samples_per_second': 122.431, 'eval_steps_per_second': 7.656, 'epoch': 0.96}\n",
      "{'train_runtime': 3121.7483, 'train_samples_per_second': 29.272, 'train_steps_per_second': 1.83, 'train_loss': 0.32360762641543434, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train('seq2seq', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=1,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hurt you and I want to find out who that was.\n",
      "Detoxified: you're gonna have to find out who's doing it.\n",
      "\n",
      "Original: I can't believe we haven't fucked for two years, nine months, three weeks and... 69 hours.\n",
      "Detoxified: I can't believe we've been a couple of years, nine months, three weeks and... 69 hours.\n",
      "\n",
      "Original: So forgive me for being a little fidgety, but if it's lunch we're talking, I'm gonna eat a fat pork sandwich, and I'm sure as shit not gonna eat it here.\n",
      "Detoxified: I'm sorry for being a little a little a bit a bit, but if it's lunch we're talking, I'll eat a pork sandwich, and I'm sure I'll not eat it here.\n",
      "\n",
      "Original: There is no fucking soy milk!\n",
      "Detoxified: there's no milk!\n",
      "\n",
      "Original: What's wrong with people having sex?\n",
      "Detoxified: what's wrong with people having sex?\n",
      "\n",
      "Original: What the fuck are you talking about?\n",
      "Detoxified: what are you talking about?\n",
      "\n",
      "Original: There were fewer horses that needed stealing than men who needed killing, after all.\n",
      "Detoxified: there were less horses that needed to steal than men who needed to kill.\n",
      "\n",
      "Original: Hey, Bob, thanks for giving us a chance to grieve together. Shit!\n",
      "Detoxified: thanks for giving us a chance to grieve together.\n",
      "\n",
      "Original: But my landlord's another fucking story-- Trying to kick me out of my home.\n",
      "Detoxified: but my landlord's another story, trying to get me out of my home.\n",
      "\n",
      "Original: Gryilus, the father of young Phylon. I slashed his face with a whip.\n",
      "Detoxified: Gryilus, the father of Phylon, I slashed his face with a whip.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = AutoModelForSeq2SeqLM.from_pretrained(\"../models/detoxificator\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/detoxificator\")\n",
    "\n",
    "test_toxic_sentences = pd.read_csv('../data/interim/test.csv')['reference'].to_list()[:10]\n",
    "\n",
    "detoxified = detoxificate_style_transfer(test_toxic_sentences, best_model, tokenizer)\n",
    "\n",
    "for sentence, detoxified_sentence in zip(test_toxic_sentences, detoxified):\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Detoxified: {detoxified_sentence}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

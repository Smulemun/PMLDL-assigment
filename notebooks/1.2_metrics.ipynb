{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "### This notebook introduces metrics used for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.models.metrics import semantic_similarity, style_accuracy, fluency, j_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an example on which the metrics will be demonstrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: They're all laughing at us, so we'll kick your ass.\n",
      "\n",
      "Non-toxic text: they're laughing at us. We'll show you.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/filtered.csv')\n",
    "df = df.iloc[0]\n",
    "pred, label = df['reference'], df['translation']\n",
    "print(f'Original text: {pred}\\n\\nNon-toxic text: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity shows how similar two texts are based on their meaning (1 - same meaning, 0 - different meaning). In our problem it is important to not lose the meaning when removing the toxicity from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity: 0.75373\n"
     ]
    }
   ],
   "source": [
    "similarity = semantic_similarity([pred], [label])\n",
    "\n",
    "print(f'Semantic similarity: {similarity:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style accuracy shows how toxic the text is (1 - non-toxic, 0 - toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style Accuracy of original: 0.00073\n",
      "\n",
      "Style Accuracy of non-toxic: 0.99966\n"
     ]
    }
   ],
   "source": [
    "original_accuracy = style_accuracy([pred])\n",
    "non_toxic_accuracy = style_accuracy([label])\n",
    "\n",
    "print(f'Style Accuracy of original: {original_accuracy:.5f}\\n\\nStyle Accuracy of non-toxic: {non_toxic_accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluency shows how gramatically correct the given text is accordinng to the english grammar (1 - correct, 0 - incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency of original: 0.99038\n",
      "\n",
      "Fluency of non-toxic: 0.99189\n"
     ]
    }
   ],
   "source": [
    "original_fluency = fluency([pred])\n",
    "non_toxic_fluency = fluency([label])\n",
    "\n",
    "print(f'Fluency of original: {original_fluency:.5f}\\n\\nFluency of non-toxic: {non_toxic_fluency:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J metric is a combination of three metrics listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Metric of original: 0.00054\n",
      "\n",
      "J-Metric of non-toxic: 0.74736\n"
     ]
    }
   ],
   "source": [
    "original_j = j_metric(similarity, original_accuracy, original_fluency)\n",
    "non_toxic_j = j_metric(similarity, non_toxic_accuracy, non_toxic_fluency)\n",
    "\n",
    "print(f'J-Metric of original: {original_j:.5f}\\n\\nJ-Metric of non-toxic: {non_toxic_j:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

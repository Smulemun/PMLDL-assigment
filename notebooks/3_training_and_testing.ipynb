{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/filtered.tsv', sep='\\t')\n",
    "sents = df[(df['similarity'] < 0.7) & (df['ref_tox'] > df['trn_tox'])]\n",
    "sents = sents[['reference', 'translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "class DetoxDataset(Dataset):\n",
    "    def __init__(self, sents, train=True, vocab=None):\n",
    "        self.tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "        self.train = train\n",
    "        self.sents = [sent[0] for sent in sents.values]\n",
    "        if self.train:\n",
    "            self.labels = [sent[1] for sent in sents.values]\n",
    "        else:\n",
    "            self.labels = []\n",
    "        if vocab is None:\n",
    "            self.vocab = self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = build_vocab_from_iterator(map(self.tokenizer, self.sents + self.labels),\n",
    "                                          min_freq=1,\n",
    "                                          specials=special_symbols,\n",
    "                                          special_first=True)\n",
    "        vocab.set_default_index(UNK_IDX)\n",
    "        return vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def get_tokens(self, sentence):\n",
    "        tokens = self.tokenizer(sentence)\n",
    "        return self.vocab(tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.get_tokens(self.sents[idx]), self.get_tokens(self.labels[idx])\n",
    "        else:\n",
    "            return self.get_tokens(self.sents[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DetoxDataset(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIZE = 30\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sentences_batch, labels_batch = [], []\n",
    "    sentences_padding_mask, labels_padding_mask = [], []\n",
    "    for _sent, _label in batch:\n",
    "        _sent = _sent[:MAX_SIZE]\n",
    "        _label = _label[:MAX_SIZE]\n",
    "\n",
    "        sent_mask = [1] * (len(_sent) + 2)\n",
    "        label_mask = [1] * (len(_label) + 2)\n",
    "\n",
    "        _sent.append(EOS_IDX)\n",
    "        while len(_sent) < MAX_SIZE + 1:\n",
    "            _sent.append(PAD_IDX)\n",
    "            sent_mask.append(0)\n",
    "        sentences_batch.append(torch.cat((torch.tensor([BOS_IDX]),\n",
    "                                          torch.tensor(_sent))))\n",
    "        sentences_padding_mask.append(torch.tensor(sent_mask))\n",
    "\n",
    "        _label.append(EOS_IDX)\n",
    "        while len(_label) < MAX_SIZE + 1:\n",
    "            _label.append(PAD_IDX)\n",
    "            label_mask.append(0)\n",
    "        labels_batch.append(torch.cat((torch.tensor([BOS_IDX]),\n",
    "                                          torch.tensor(_label))))\n",
    "        labels_padding_mask.append(torch.tensor(label_mask))\n",
    "\n",
    "    sentences_batch = torch.stack(sentences_batch, dim=0)\n",
    "    labels_batch = torch.stack(labels_batch, dim=0)\n",
    "\n",
    "    senteces_mask = torch.zeros((len(batch) * 8, MAX_SIZE + 2, MAX_SIZE + 2)).type(torch.bool)\n",
    "    labels_mask = torch.zeros((len(batch) * 8, MAX_SIZE + 2, MAX_SIZE + 2)).type(torch.bool)\n",
    "\n",
    "    sentences_padding_mask = torch.stack(sentences_padding_mask, dim=0).type(torch.bool)\n",
    "    labels_padding_mask = torch.stack(labels_padding_mask, dim=0).type(torch.bool)\n",
    "\n",
    "    return sentences_batch, labels_batch, senteces_mask, labels_mask, sentences_padding_mask, labels_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53709\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     sent, label, src_padding_mask, tgt_padding_mask = batch\n",
    "#     print(sent[0], src_padding_mask[0], sep='\\n')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 emb_size=512,\n",
    "                 nhead=8,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, vocab_size)\n",
    "        self.tok_embedding = TokenEmbedding(vocab_size, emb_size)\n",
    "        self.pos_encoder = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                sentence,\n",
    "                target,\n",
    "                senteces_mask, \n",
    "                targets_mask,\n",
    "                sentence_padding_mask,\n",
    "                target_padding_mask,\n",
    "                ):\n",
    "        sentence_embedding = self.tok_embedding(sentence)\n",
    "        target_embedding = self.tok_embedding(target)\n",
    "        sentence_embedding = self.pos_encoder(sentence_embedding)\n",
    "        target_embedding = self.pos_encoder(target_embedding)\n",
    "        outs = self.transformer(sentence_embedding,\n",
    "                                target_embedding, \n",
    "                                senteces_mask,\n",
    "                                targets_mask,\n",
    "                                src_key_padding_mask=sentence_padding_mask, \n",
    "                                tgt_key_padding_mask=target_padding_mask\n",
    "                                )\n",
    "        return self.generator(outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(vocab_size=len(dataset.vocab), num_encoder_layers=2, num_decoder_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, criterion, device, train_loader):\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch in pbar:\n",
    "        # torch.cuda.empty_cache()\n",
    "        sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = batch\n",
    "        sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = sent.to(device), label.to(device), senteces_mask.to(device), labels_mask.to(device), src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "\n",
    "        output = model(sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # print(output.shape)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if np.isnan(loss.item()):\n",
    "        #     print(loss.item())\n",
    "        #     print(output)\n",
    "        #     print('---')\n",
    "        #     print(label)\n",
    "            print(losses)\n",
    "            break\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pbar.set_description(f'Epoch {epoch}: Avg Loss: {np.mean(losses):.5f}')\n",
    "\n",
    "def evaluate(epoch, model, criterion, device, val_loader):\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    pbar = tqdm(val_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = batch\n",
    "            sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = sent.to(device), label.to(device), senteces_mask.to(device), labels_mask.to(device), src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "\n",
    "            output = model(sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask)\n",
    "            output = output.permute(0, 2, 1)\n",
    "            # print(output.shape)\n",
    "            loss = criterion(output, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_description(f'\\tEpoch {epoch}: Avg Val Loss: {np.mean(losses):.5f}')\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            sent, target, src_padding_mask, tgt_padding_mask = batch\n",
    "            sent, target = sent[0], target[0]\n",
    "            print(' '.join(dataset.vocab.lookup_tokens(sent.tolist())))\n",
    "            print(' '.join(dataset.vocab.lookup_tokens(target.tolist())))\n",
    "            pred = predict(sent, model, device)\n",
    "            print(' '.join(dataset.vocab.lookup_tokens(pred[0].tolist())))\n",
    "            break\n",
    "        \n",
    "def predict(batch, model, device):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    sent, _, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = batch\n",
    "\n",
    "    sent = sent[0].to(device).reshape(1, -1)\n",
    "    label = torch.tensor([PAD_IDX]).to(device).reshape(1, -1)\n",
    "    senteces_mask = senteces_mask[0].to(device).reshape(1, -1)\n",
    "    labels_mask = torch.zeros((8, len(label), len(label))).to(device).type(torch.bool)\n",
    "    src_padding_mask = src_padding_mask[0].to(device).reshape(1, -1)\n",
    "    tgt_padding_mask = torch.ones((1, len(label))).to(device).type(torch.bool)\n",
    "    model.eval()\n",
    "    print(senteces_mask.shape)\n",
    "    print(labels_mask.shape)\n",
    "    print(src_padding_mask.shape)\n",
    "    print(tgt_padding_mask.shape)\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_SIZE):\n",
    "            output = model(sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask)\n",
    "            output = output.permute(0, 2, 1)\n",
    "            token = torch.argmax(output[0, :, i]).item()\n",
    "            label = torch.tensor(label[0].tolist() + [token]).to(device).reshape(1, -1)\n",
    "            labels_mask = torch.zeros((8, len(label), len(label))).to(device).type(torch.bool)\n",
    "            tgt_padding_mask = torch.ones((1, len(label))).to(device).type(torch.bool)\n",
    "            if token == EOS_IDX:\n",
    "                break\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> You ever had a pissed - off marine on your ass ? <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<bos> you ever been pissed off by a Marine ? <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "c:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 1, 2]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(dataset\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mlookup_tokens(sent\u001b[39m.\u001b[39mtolist())))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(dataset\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mlookup_tokens(target\u001b[39m.\u001b[39mtolist())))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred \u001b[39m=\u001b[39m predict(batch, model, \u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(dataset\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mlookup_tokens(pred[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist())))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_SIZE):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         output \u001b[39m=\u001b[39m model(sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output[\u001b[39m0\u001b[39m, :, i])\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m sentence_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(sentence_embedding)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m target_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(target_embedding)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(sentence_embedding,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                         target_embedding, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                         senteces_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                         targets_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                         src_key_padding_mask\u001b[39m=\u001b[39;49msentence_padding_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                         tgt_key_padding_mask\u001b[39m=\u001b[39;49mtarget_padding_mask\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(outs)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, mask\u001b[39m=\u001b[39msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m--> 146\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask, memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    148\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    366\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    368\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 369\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    370\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    371\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    372\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:716\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    714\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x))\n\u001b[0;32m    715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m    717\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    718\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:725\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    724\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    726\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    727\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    728\u001b[0m                        is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[0;32m    729\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    730\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1158\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1155\u001b[0m         why_not_fast_path \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mgrad is enabled and at least one of query or the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1156\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39minput/output projection weights or biases requires_grad\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m why_not_fast_path:\n\u001b[1;32m-> 1158\u001b[0m         merged_mask, mask_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmerge_masks(attn_mask, key_padding_mask, query)\n\u001b[0;32m   1160\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_native_multi_head_attention(\n\u001b[0;32m   1161\u001b[0m             query,\n\u001b[0;32m   1162\u001b[0m             key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1172\u001b[0m             average_attn_weights,\n\u001b[0;32m   1173\u001b[0m             mask_type)\n\u001b[0;32m   1175\u001b[0m any_nested \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mis_nested \u001b[39mor\u001b[39;00m key\u001b[39m.\u001b[39mis_nested \u001b[39mor\u001b[39;00m value\u001b[39m.\u001b[39mis_nested\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1265\u001b[0m, in \u001b[0;36mMultiheadAttention.merge_masks\u001b[1;34m(self, attn_mask, key_padding_mask, query)\u001b[0m\n\u001b[0;32m   1262\u001b[0m     merged_mask \u001b[39m=\u001b[39m attn_mask_expanded\n\u001b[0;32m   1264\u001b[0m     \u001b[39mif\u001b[39;00m key_padding_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1265\u001b[0m         key_padding_mask_expanded \u001b[39m=\u001b[39m key_padding_mask\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, seq_len)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   1266\u001b[0m         merged_mask \u001b[39m=\u001b[39m attn_mask_expanded \u001b[39m+\u001b[39m key_padding_mask_expanded\n\u001b[0;32m   1268\u001b[0m \u001b[39m# no attn_mask and no key_padding_mask, returns None, None\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 1, 2]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "for batch in val_loader:\n",
    "    sent, target, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask = batch\n",
    "    sent, target = sent[0], target[0]\n",
    "    print(' '.join(dataset.vocab.lookup_tokens(sent.tolist())))\n",
    "    print(' '.join(dataset.vocab.lookup_tokens(target.tolist())))\n",
    "    pred = predict(batch, model, \"cuda\")\n",
    "    print(' '.join(dataset.vocab.lookup_tokens(pred[0].tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11423 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: Avg Val Loss: nan:   7%|▋         | 89/1270 [00:02<00:27, 42.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(epoch, model, optimizer, criterion, \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m, train_loader)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     evaluate(epoch, model, criterion, \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m, val_loader)\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask \u001b[39m=\u001b[39m sent\u001b[39m.\u001b[39mto(device), label\u001b[39m.\u001b[39mto(device), senteces_mask\u001b[39m.\u001b[39mto(device), labels_mask\u001b[39m.\u001b[39mto(device), src_padding_mask\u001b[39m.\u001b[39mto(device), tgt_padding_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m output \u001b[39m=\u001b[39m model(sent, label, senteces_mask, labels_mask, src_padding_mask, tgt_padding_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(output.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_training_and_testing.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m sentence_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(sentence_embedding)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m target_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(target_embedding)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(sentence_embedding,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                         target_embedding, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                         senteces_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                         targets_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                         src_key_padding_mask\u001b[39m=\u001b[39;49msentence_padding_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                         tgt_key_padding_mask\u001b[39m=\u001b[39;49mtarget_padding_mask\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_training_and_testing.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(outs)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 145\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    146\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    148\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[0;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:562\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[0;32m    561\u001b[0m         merged_mask, mask_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn\u001b[39m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[1;32m--> 562\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_transformer_encoder_layer_fwd(\n\u001b[0;32m    563\u001b[0m             src,\n\u001b[0;32m    564\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49membed_dim,\n\u001b[0;32m    565\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m    566\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49min_proj_weight,\n\u001b[0;32m    567\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m    568\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    569\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    570\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_relu_or_gelu \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[0;32m    571\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_first,\n\u001b[0;32m    572\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    573\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    574\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    575\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    576\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    577\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    578\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    579\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear2\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    580\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear2\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    581\u001b[0m             merged_mask,\n\u001b[0;32m    582\u001b[0m             mask_type,\n\u001b[0;32m    583\u001b[0m         )\n\u001b[0;32m    586\u001b[0m x \u001b[39m=\u001b[39m src\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_first:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(epoch, model, optimizer, criterion, 'cuda', train_loader)\n",
    "    evaluate(epoch, model, criterion, 'cuda', val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d259e6e450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import put_mask_with_classifier, get_toxicity\n",
    "from src.models.predict import detoxificate_text_with_classifier\n",
    "from src.models.train import train, train_classifier, evaluate_classifier\n",
    "from src.models.metrics import semantic_similarity, style_accuracy, fluency, j_metric\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../models/bert_maskedlm\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5038/5038 [00:00<00:00, 8189.42it/s]\n",
      "100%|██████████| 1904/1904 [00:00<00:00, 6141.47it/s]\n"
     ]
    }
   ],
   "source": [
    "class ToxicWordsDataset(TorchDataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        positive_words = open('../data/external/positive_words.txt').read().split('\\n')\n",
    "        negative_words = open('../data/external/negative_words.txt').read().split('\\n')\n",
    "        toxic_words = open('../data/external/toxic_words.txt').read().split('\\n')\n",
    "        toxic_words.extend(negative_words)\n",
    "\n",
    "        toxic_words = [w for w in toxic_words if w.isalnum() and len(w) > 1]\n",
    "        positive_words = [w for w in positive_words if w.isalnum() and len(w) > 1]\n",
    "\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for w in tqdm(toxic_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(1)\n",
    "\n",
    "        for w in tqdm(positive_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "    \n",
    "dataset = ToxicWordsDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicWordsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "toxicity_classifier = ToxicWordsClassifier(dataset.tokenizer.vocab_size, 512, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6442, Acc: 0.5714: 100%|██████████| 196/196 [00:04<00:00, 41.40it/s] \n",
      "\tEpoch: 0, Loss: 0.5073, Acc: 0.5217: 100%|██████████| 22/22 [00:00<00:00, 360.64it/s]\n",
      "Epoch: 1, Loss: 0.3454, Acc: 0.5714: 100%|██████████| 196/196 [00:01<00:00, 103.82it/s]\n",
      "\tEpoch: 1, Loss: 0.8103, Acc: 0.4348: 100%|██████████| 22/22 [00:00<00:00, 274.83it/s]\n",
      "Epoch: 2, Loss: 0.5386, Acc: 0.7143: 100%|██████████| 196/196 [00:01<00:00, 106.37it/s]\n",
      "\tEpoch: 2, Loss: 0.5647, Acc: 0.6957: 100%|██████████| 22/22 [00:00<00:00, 314.26it/s]\n",
      "Epoch: 3, Loss: 0.4277, Acc: 0.7143: 100%|██████████| 196/196 [00:01<00:00, 107.63it/s]\n",
      "\tEpoch: 3, Loss: 0.6629, Acc: 0.3913: 100%|██████████| 22/22 [00:00<00:00, 349.18it/s]\n",
      "Epoch: 4, Loss: 0.1066, Acc: 1.0000: 100%|██████████| 196/196 [00:01<00:00, 107.33it/s]\n",
      "\tEpoch: 4, Loss: 0.7784, Acc: 0.5652: 100%|██████████| 22/22 [00:00<00:00, 349.20it/s]\n",
      "Epoch: 5, Loss: 0.2370, Acc: 0.8571: 100%|██████████| 196/196 [00:01<00:00, 105.61it/s]\n",
      "\tEpoch: 5, Loss: 0.7972, Acc: 0.5217: 100%|██████████| 22/22 [00:00<00:00, 333.30it/s]\n",
      "Epoch: 6, Loss: 0.2196, Acc: 1.0000: 100%|██████████| 196/196 [00:01<00:00, 107.21it/s]\n",
      "\tEpoch: 6, Loss: 0.6638, Acc: 0.6957: 100%|██████████| 22/22 [00:00<00:00, 366.63it/s]\n",
      "Epoch: 7, Loss: 0.0590, Acc: 1.0000: 100%|██████████| 196/196 [00:01<00:00, 107.10it/s]\n",
      "\tEpoch: 7, Loss: 0.3285, Acc: 0.7391: 100%|██████████| 22/22 [00:00<00:00, 349.20it/s]\n",
      "Epoch: 8, Loss: 0.1336, Acc: 0.7143: 100%|██████████| 196/196 [00:01<00:00, 108.14it/s]\n",
      "\tEpoch: 8, Loss: 0.4745, Acc: 0.6957: 100%|██████████| 22/22 [00:00<00:00, 271.59it/s]\n",
      "Epoch: 9, Loss: 0.5159, Acc: 0.7143: 100%|██████████| 196/196 [00:01<00:00, 105.82it/s]\n",
      "\tEpoch: 9, Loss: 0.5342, Acc: 0.7826: 100%|██████████| 22/22 [00:00<00:00, 333.31it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(toxicity_classifier.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_classifier(epoch, toxicity_classifier, optimizer, criterion, train_dataloader, device)\n",
    "    evaluate_classifier(epoch, toxicity_classifier, criterion, val_dataloader, device)\n",
    "\n",
    "torch.save(toxicity_classifier.state_dict(), '../models/toxicity_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: buttcheeks, Toxicity: 0.9843496680259705\n",
      "Word: university, Toxicity: 0.15710808336734772\n"
     ]
    }
   ],
   "source": [
    "toxic_word = 'buttcheeks'\n",
    "non_toxic_word = 'university'\n",
    "print(f'Word: {toxic_word}, Toxicity: {get_toxicity(toxic_word, tokenizer, toxicity_classifier)}')\n",
    "print(f'Word: {non_toxic_word}, Toxicity: {get_toxicity(non_toxic_word, tokenizer, toxicity_classifier)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11211/101535 [00:39<04:45, 316.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101535/101535 [06:36<00:00, 255.96it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/filtered.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(toxic_sentences))):\n",
    "    toxic_sentences[i] = put_mask_with_classifier(toxic_sentences[i], tokenizer, toxicity_classifier)\n",
    "    if '[MASK]' in toxic_sentences[i]:\n",
    "        data.append(toxic_sentences[i])\n",
    "        labels.append(non_toxic_sentences[i])\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71202f319af149c881a2cfbcd0422492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180819d474a74b508c7c836ab8e4ca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4868, 'learning_rate': 1.8229775181448045e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5583e79fe849455bba56d22d32bc4ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.63 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 0 bytes free; 4.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\3_maskedlm_with_classifier.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(\u001b[39m'\u001b[39;49m\u001b[39mmaskedlm\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       model, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       tokenizer, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m       train_dataset, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       val_dataset, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m       data_collator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m       batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m       epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m       seed\u001b[39m=\u001b[39;49mRANDOM_SEED\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/3_maskedlm_with_classifier.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\..\\src\\models\\train.py:49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model_type, model, tokenizer, train_dataset, val_dataset, data_collator, batch_size, epochs, seed)\u001b[0m\n\u001b[0;32m     29\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     30\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../models\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     41\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     42\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[0;32m     48\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     50\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39m../models/bert_maskedlm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39m../models/bert_maskedlm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2020\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2017\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2018\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 2020\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2021\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2022\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2321\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2319\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2321\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2324\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3053\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3050\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   3052\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3053\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   3054\u001b[0m     eval_dataloader,\n\u001b[0;32m   3055\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3056\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3057\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3058\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   3059\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   3060\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   3061\u001b[0m )\n\u001b[0;32m   3063\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   3064\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3270\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3268\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[0;32m   3269\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(logits)\n\u001b[1;32m-> 3270\u001b[0m     preds_host \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m   3271\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3272\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(labels)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:116\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[0;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, Mapping):\n\u001b[0;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[0;32m    119\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    120\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:75\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[1;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[0;32m     72\u001b[0m tensor2 \u001b[39m=\u001b[39m atleast_1d(tensor2)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tensor1\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat((tensor1, tensor2), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     77\u001b[0m \u001b[39m# Let's figure out the new shape\u001b[39;00m\n\u001b[0;32m     78\u001b[0m new_shape \u001b[39m=\u001b[39m (tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m tensor2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m tensor1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.63 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 0 bytes free; 4.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train('maskedlm', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=1,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Suddenly, to the delight and outrage of the congregation, a raucous saxophone broke the solemnity, and a jazz rendering of \"Fools Rush In\" was blaring over the loudspeakers.\n",
      "Masked: [MASK] [MASK] the delight and [MASK] [MASK] the congregation, [MASK] [MASK] [MASK] [MASK] the [MASK] and [MASK] jazz rendering [MASK] \"fools [MASK] [MASK] [MASK] [MASK] [MASK] the [MASK]\n",
      "Detoxified: and, the delight and joy from the congregation, the the ed, the blues and the jazz renderings \" fools, \" \" \" and the world\n",
      "\n",
      "Original: This place is such a dump.\n",
      "Masked: this place is such [MASK] [MASK]\n",
      "Detoxified: this place is such fun!\n",
      "\n",
      "Original: Doesn't mean a damn thing!\n",
      "Masked: doesn't [MASK] [MASK] [MASK] thing!\n",
      "Detoxified: doesn't know the ing thing!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "best_model = BertForMaskedLM.from_pretrained(\"../models/bert_maskedlm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../models/bert_maskedlm\")\n",
    "\n",
    "random_toxic_sentences = random.sample(df['reference'].tolist(), 3)\n",
    "\n",
    "for sentence in random_toxic_sentences:\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Masked: {put_mask_with_classifier(sentence, tokenizer, toxicity_classifier)}')\n",
    "    print(f'Detoxified: {detoxificate_text_with_classifier(sentence, tokenizer, best_model, toxicity_classifier)}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

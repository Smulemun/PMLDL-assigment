{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x189a5bce450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import put_mask_with_classifier, get_toxicity\n",
    "from src.models.predict import detoxificate_text_with_classifier\n",
    "from src.models.train import train, train_classifier, evaluate_classifier\n",
    "from src.models.classifier import ToxicWordsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5017/5017 [00:00<00:00, 7707.56it/s]\n",
      "100%|██████████| 1904/1904 [00:00<00:00, 7294.50it/s]\n"
     ]
    }
   ],
   "source": [
    "class ToxicWordsDataset(TorchDataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        positive_words = open('../data/interim/positive_words.txt').read().split('\\n')\n",
    "        toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')\n",
    "\n",
    "        toxic_words = [w for w in toxic_words if w.isalnum() and len(w) > 1]\n",
    "        positive_words = [w for w in positive_words if w.isalnum() and len(w) > 1]\n",
    "\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for w in tqdm(toxic_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(1)\n",
    "\n",
    "        for w in tqdm(positive_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "    \n",
    "dataset = ToxicWordsDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_classifier = ToxicWordsClassifier(vocab_size=dataset.tokenizer.vocab_size, embedding_dim=512, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.55398, Acc: 0.49670: 100%|██████████| 196/196 [00:02<00:00, 80.93it/s]\n",
      "\tEpoch: 0, Loss: 0.52563, Acc: 0.58757: 100%|██████████| 22/22 [00:00<00:00, 297.28it/s]\n",
      "Epoch: 1, Loss: 0.43307, Acc: 0.69169: 100%|██████████| 196/196 [00:02<00:00, 92.75it/s]\n",
      "\tEpoch: 1, Loss: 0.50799, Acc: 0.66150: 100%|██████████| 22/22 [00:00<00:00, 289.45it/s]\n",
      "Epoch: 2, Loss: 0.33611, Acc: 0.78642: 100%|██████████| 196/196 [00:02<00:00, 95.28it/s]\n",
      "\tEpoch: 2, Loss: 0.53309, Acc: 0.69188: 100%|██████████| 22/22 [00:00<00:00, 301.35it/s]\n",
      "Epoch: 3, Loss: 0.26854, Acc: 0.84571: 100%|██████████| 196/196 [00:02<00:00, 92.02it/s]\n",
      "\tEpoch: 3, Loss: 0.56389, Acc: 0.71542: 100%|██████████| 22/22 [00:00<00:00, 265.04it/s]\n",
      "Epoch: 4, Loss: 0.23178, Acc: 0.87044: 100%|██████████| 196/196 [00:02<00:00, 91.84it/s]\n",
      "\tEpoch: 4, Loss: 0.57273, Acc: 0.71542: 100%|██████████| 22/22 [00:00<00:00, 301.35it/s]\n",
      "Epoch: 5, Loss: 0.20883, Acc: 0.88552: 100%|██████████| 196/196 [00:02<00:00, 93.46it/s]\n",
      "\tEpoch: 5, Loss: 0.56008, Acc: 0.72993: 100%|██████████| 22/22 [00:00<00:00, 289.45it/s]\n",
      "Epoch: 6, Loss: 0.19768, Acc: 0.89037: 100%|██████████| 196/196 [00:02<00:00, 94.20it/s]\n",
      "\tEpoch: 6, Loss: 0.56857, Acc: 0.72134: 100%|██████████| 22/22 [00:00<00:00, 314.26it/s]\n",
      "Epoch: 7, Loss: 0.18696, Acc: 0.88967: 100%|██████████| 196/196 [00:02<00:00, 93.59it/s]\n",
      "\tEpoch: 7, Loss: 0.57774, Acc: 0.71745: 100%|██████████| 22/22 [00:00<00:00, 301.34it/s]\n",
      "Epoch: 8, Loss: 0.18100, Acc: 0.89388: 100%|██████████| 196/196 [00:02<00:00, 95.79it/s]\n",
      "\tEpoch: 8, Loss: 0.57998, Acc: 0.73018: 100%|██████████| 22/22 [00:00<00:00, 301.35it/s]\n",
      "Epoch: 9, Loss: 0.17676, Acc: 0.89643: 100%|██████████| 196/196 [00:02<00:00, 93.54it/s]\n",
      "\tEpoch: 9, Loss: 0.57273, Acc: 0.72875: 100%|██████████| 22/22 [00:00<00:00, 305.54it/s]\n",
      "Epoch: 10, Loss: 0.17186, Acc: 0.89930: 100%|██████████| 196/196 [00:02<00:00, 95.18it/s]\n",
      "\tEpoch: 10, Loss: 0.58103, Acc: 0.70689: 100%|██████████| 22/22 [00:00<00:00, 285.69it/s]\n",
      "Epoch: 11, Loss: 0.16628, Acc: 0.89755: 100%|██████████| 196/196 [00:02<00:00, 93.78it/s]\n",
      "\tEpoch: 11, Loss: 0.59733, Acc: 0.72418: 100%|██████████| 22/22 [00:00<00:00, 284.60it/s]\n",
      "Epoch: 12, Loss: 0.16576, Acc: 0.89509: 100%|██████████| 196/196 [00:02<00:00, 93.85it/s]\n",
      "\tEpoch: 12, Loss: 0.58164, Acc: 0.70720: 100%|██████████| 22/22 [00:00<00:00, 289.46it/s]\n",
      "Epoch: 13, Loss: 0.16405, Acc: 0.89381: 100%|██████████| 196/196 [00:02<00:00, 96.65it/s]\n",
      "\tEpoch: 13, Loss: 0.59813, Acc: 0.71004: 100%|██████████| 22/22 [00:00<00:00, 271.58it/s]\n",
      "Epoch: 14, Loss: 0.15973, Acc: 0.89755: 100%|██████████| 196/196 [00:02<00:00, 95.00it/s]\n",
      "\tEpoch: 14, Loss: 0.59648, Acc: 0.71826: 100%|██████████| 22/22 [00:00<00:00, 255.79it/s]\n",
      "Epoch: 15, Loss: 0.15890, Acc: 0.89413: 100%|██████████| 196/196 [00:02<00:00, 94.41it/s]\n",
      "\tEpoch: 15, Loss: 0.59561, Acc: 0.72363: 100%|██████████| 22/22 [00:00<00:00, 305.53it/s]\n",
      "Epoch: 16, Loss: 0.15609, Acc: 0.89987: 100%|██████████| 196/196 [00:02<00:00, 94.72it/s]\n",
      "\tEpoch: 16, Loss: 0.59737, Acc: 0.71486: 100%|██████████| 22/22 [00:00<00:00, 252.86it/s]\n",
      "Epoch: 17, Loss: 0.15620, Acc: 0.90019: 100%|██████████| 196/196 [00:02<00:00, 94.86it/s]\n",
      "\tEpoch: 17, Loss: 0.60083, Acc: 0.70522: 100%|██████████| 22/22 [00:00<00:00, 293.31it/s]\n",
      "Epoch: 18, Loss: 0.15292, Acc: 0.89627: 100%|██████████| 196/196 [00:02<00:00, 94.04it/s]\n",
      "\tEpoch: 18, Loss: 0.59027, Acc: 0.68188: 100%|██████████| 22/22 [00:00<00:00, 282.03it/s]\n",
      "Epoch: 19, Loss: 0.15478, Acc: 0.89420: 100%|██████████| 196/196 [00:02<00:00, 95.32it/s]\n",
      "\tEpoch: 19, Loss: 0.59557, Acc: 0.71004: 100%|██████████| 22/22 [00:00<00:00, 252.85it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "optimizer = torch.optim.Adam(toxicity_classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_loss = 1e9\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_classifier(epoch, toxicity_classifier, optimizer, criterion, train_dataloader, device)\n",
    "    loss = evaluate_classifier(epoch, toxicity_classifier, criterion, val_dataloader, device)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        torch.save(toxicity_classifier.state_dict(), '../models/toxicity_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: buttcheeks, Toxicity: 0.8885523676872253\n",
      "Word: university, Toxicity: 0.5311500430107117\n"
     ]
    }
   ],
   "source": [
    "toxic_word = 'buttcheeks'\n",
    "non_toxic_word = 'university'\n",
    "\n",
    "toxicity_classifier.load_state_dict(torch.load('../models/toxicity_classifier.pth'))\n",
    "\n",
    "print(f'Word: {toxic_word}, Toxicity: {get_toxicity(toxic_word, tokenizer, toxicity_classifier)}')\n",
    "print(f'Word: {non_toxic_word}, Toxicity: {get_toxicity(non_toxic_word, tokenizer, toxicity_classifier)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101535/101535 [08:05<00:00, 209.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(toxic_sentences))):\n",
    "    toxic_sentences[i] = put_mask_with_classifier(toxic_sentences[i], tokenizer, toxicity_classifier)\n",
    "    if '[MASK]' in toxic_sentences[i]:\n",
    "        data.append(toxic_sentences[i])\n",
    "        labels.append(non_toxic_sentences[i])\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4652afbd19664311be73aff3709d4b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0301ed481640494d814615443068cb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2831, 'learning_rate': 1.819037278320666e-05, 'epoch': 0.09}\n",
      "{'loss': 3.0637, 'learning_rate': 1.638074556641332e-05, 'epoch': 0.18}\n",
      "{'loss': 3.0873, 'learning_rate': 1.4571118349619979e-05, 'epoch': 0.27}\n",
      "{'loss': 2.9544, 'learning_rate': 1.2761491132826638e-05, 'epoch': 0.36}\n",
      "{'loss': 2.9274, 'learning_rate': 1.0951863916033298e-05, 'epoch': 0.45}\n",
      "{'loss': 2.9715, 'learning_rate': 9.142236699239957e-06, 'epoch': 0.54}\n",
      "{'loss': 2.8911, 'learning_rate': 7.332609482446616e-06, 'epoch': 0.63}\n",
      "{'loss': 2.8747, 'learning_rate': 5.5229822656532765e-06, 'epoch': 0.72}\n",
      "{'loss': 2.8676, 'learning_rate': 3.7133550488599353e-06, 'epoch': 0.81}\n",
      "{'loss': 2.8604, 'learning_rate': 1.9037278320665944e-06, 'epoch': 0.9}\n",
      "{'loss': 2.8661, 'learning_rate': 9.410061527325373e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5a650710fb4ec4a84e7025a3ce7b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.749858856201172, 'eval_runtime': 157.18, 'eval_samples_per_second': 62.495, 'eval_steps_per_second': 3.906, 'epoch': 1.0}\n",
      "{'train_runtime': 3724.7467, 'train_samples_per_second': 23.735, 'train_steps_per_second': 1.484, 'train_loss': 2.9680513250107268, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train('maskedlm_with_classifier', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=1,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Suddenly, to the delight and outrage of the congregation, a raucous saxophone broke the solemnity, and a jazz rendering of \"Fools Rush In\" was blaring over the loudspeakers.\n",
      "Masked: suddenly, to [MASK] delight and outrage of [MASK] [MASK] a raucous saxophone [MASK] [MASK] [MASK] and a jazz rendering of [MASK] rush [MASK] was [MASK] [MASK] [MASK] loudspeakers.\n",
      "Detoxified: suddenly, to a delight and outrage of all, a raucous saxophone struck a, and a jazz rendering of a rush hour was played on on loudspeakers.\n",
      "\n",
      "Original: This place is such a dump.\n",
      "Masked: this [MASK] [MASK] such a [MASK]\n",
      "Detoxified: this guy not such a pain\n",
      "\n",
      "Original: Doesn't mean a damn thing!\n",
      "Masked: doesn't mean a [MASK] thing!\n",
      "Detoxified: doesn't mean a damned thing!\n",
      "\n",
      "Original: I’m just going to have to find someone to cover for my ass first.’\n",
      "Masked: i’m just going to have to find someone to [MASK] [MASK] [MASK] [MASK] first.’\n",
      "Detoxified: i ’ m just going to have to find someone to kick me to to first. ’\n",
      "\n",
      "Original: He is a walking dead man with no will of his own.\n",
      "Masked: [MASK] [MASK] a [MASK] [MASK] [MASK] [MASK] no [MASK] of [MASK] own.\n",
      "Detoxified: i was a little of who had no business of her own.\n",
      "\n",
      "Original: You're such a jerk.\n",
      "Masked: you're such a [MASK]\n",
      "Detoxified: you're such a pussy\n",
      "\n",
      "Original: I'm jacking off.\n",
      "Masked: i'm [MASK] [MASK]\n",
      "Detoxified: i'm a!\n",
      "\n",
      "Original: I may puke.\n",
      "Masked: i may [MASK]\n",
      "Detoxified: i may not\n",
      "\n",
      "Original: You gotta be shitting me.\n",
      "Masked: you [MASK] [MASK] [MASK] me.\n",
      "Detoxified: you have to at me.\n",
      "\n",
      "Original: You're a great liar, Dad.\n",
      "Masked: you're a great [MASK] dad.\n",
      "Detoxified: you're a great little dad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "best_model = BertForMaskedLM.from_pretrained(\"../models/bert_maskedlm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../models/bert_maskedlm\")\n",
    "\n",
    "random_toxic_sentences = random.sample(df['reference'].tolist(), 10)\n",
    "\n",
    "for sentence in random_toxic_sentences:\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Masked: {put_mask_with_classifier(sentence, tokenizer, toxicity_classifier)}')\n",
    "    print(f'Detoxified: {detoxificate_text_with_classifier(sentence, tokenizer, best_model, toxicity_classifier)}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

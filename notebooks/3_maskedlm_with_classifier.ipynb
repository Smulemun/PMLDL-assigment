{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Hypothesis: Mask toxic words using classifier and then use MaskedLM to find appropriate alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e11dcde430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import put_mask_with_classifier, get_toxicity\n",
    "from src.models.predict import detoxificate_text_with_classifier\n",
    "from src.models.train import train, train_classifier, evaluate_classifier\n",
    "from src.models.classifier import ToxicWordsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading bert-base-uncased model for MaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset class for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5017/5017 [00:00<00:00, 7624.08it/s]\n",
      "100%|██████████| 1904/1904 [00:00<00:00, 5987.00it/s]\n"
     ]
    }
   ],
   "source": [
    "class ToxicWordsDataset(TorchDataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        positive_words = open('../data/interim/positive_words.txt').read().split('\\n')\n",
    "        toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')\n",
    "\n",
    "        toxic_words = [w for w in toxic_words if w.isalnum() and len(w) > 1]\n",
    "        positive_words = [w for w in positive_words if w.isalnum() and len(w) > 1]\n",
    "\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for w in tqdm(toxic_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(1)\n",
    "\n",
    "        for w in tqdm(positive_words):\n",
    "            word = self.tokenizer(w, add_special_tokens=False, max_length=1, truncation=True).input_ids\n",
    "            self.texts.append(word[0])\n",
    "            self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "    \n",
    "dataset = ToxicWordsDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the classifier data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_classifier = ToxicWordsClassifier(vocab_size=dataset.tokenizer.vocab_size, embedding_dim=512, dropout=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier and saving the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.57610, Acc: 0.46808: 100%|██████████| 195/195 [00:02<00:00, 86.62it/s]\n",
      "\tEpoch: 0, Loss: 0.57708, Acc: 0.49330: 100%|██████████| 22/22 [00:00<00:00, 205.59it/s]\n",
      "Epoch: 1, Loss: 0.49233, Acc: 0.60946: 100%|██████████| 195/195 [00:02<00:00, 96.34it/s]\n",
      "\tEpoch: 1, Loss: 0.56154, Acc: 0.60768: 100%|██████████| 22/22 [00:00<00:00, 265.05it/s]\n",
      "Epoch: 2, Loss: 0.42995, Acc: 0.69635: 100%|██████████| 195/195 [00:01<00:00, 97.93it/s] \n",
      "\tEpoch: 2, Loss: 0.54478, Acc: 0.64969: 100%|██████████| 22/22 [00:00<00:00, 293.31it/s]\n",
      "Epoch: 3, Loss: 0.36888, Acc: 0.75769: 100%|██████████| 195/195 [00:01<00:00, 98.28it/s] \n",
      "\tEpoch: 3, Loss: 0.54644, Acc: 0.64678: 100%|██████████| 22/22 [00:00<00:00, 265.04it/s]\n",
      "Epoch: 4, Loss: 0.32649, Acc: 0.79827: 100%|██████████| 195/195 [00:02<00:00, 95.16it/s] \n",
      "\tEpoch: 4, Loss: 0.56609, Acc: 0.65165: 100%|██████████| 22/22 [00:00<00:00, 309.83it/s]\n",
      "Epoch: 5, Loss: 0.28430, Acc: 0.83304: 100%|██████████| 195/195 [00:02<00:00, 96.86it/s]\n",
      "\tEpoch: 5, Loss: 0.57048, Acc: 0.66599: 100%|██████████| 22/22 [00:00<00:00, 305.53it/s]\n",
      "Epoch: 6, Loss: 0.26494, Acc: 0.84981: 100%|██████████| 195/195 [00:02<00:00, 96.82it/s] \n",
      "\tEpoch: 6, Loss: 0.57934, Acc: 0.69433: 100%|██████████| 22/22 [00:00<00:00, 297.27it/s]\n",
      "Epoch: 7, Loss: 0.23935, Acc: 0.86170: 100%|██████████| 195/195 [00:02<00:00, 95.49it/s]\n",
      "\tEpoch: 7, Loss: 0.58833, Acc: 0.68236: 100%|██████████| 22/22 [00:00<00:00, 309.83it/s]\n",
      "Epoch: 8, Loss: 0.22560, Acc: 0.87154: 100%|██████████| 195/195 [00:01<00:00, 97.79it/s]\n",
      "\tEpoch: 8, Loss: 0.61826, Acc: 0.68506: 100%|██████████| 22/22 [00:00<00:00, 274.99it/s]\n",
      "Epoch: 9, Loss: 0.21118, Acc: 0.87987: 100%|██████████| 195/195 [00:01<00:00, 98.43it/s]\n",
      "\tEpoch: 9, Loss: 0.63185, Acc: 0.69298: 100%|██████████| 22/22 [00:00<00:00, 309.84it/s]\n",
      "Epoch: 10, Loss: 0.20734, Acc: 0.88135: 100%|██████████| 195/195 [00:02<00:00, 97.35it/s]\n",
      "\tEpoch: 10, Loss: 0.62577, Acc: 0.67228: 100%|██████████| 22/22 [00:00<00:00, 301.35it/s]\n",
      "Epoch: 11, Loss: 0.19395, Acc: 0.88971: 100%|██████████| 195/195 [00:02<00:00, 97.37it/s]\n",
      "\tEpoch: 11, Loss: 0.64716, Acc: 0.69163: 100%|██████████| 22/22 [00:00<00:00, 305.54it/s]\n",
      "Epoch: 12, Loss: 0.19444, Acc: 0.88615: 100%|██████████| 195/195 [00:02<00:00, 93.90it/s]\n",
      "\tEpoch: 12, Loss: 0.65496, Acc: 0.69508: 100%|██████████| 22/22 [00:00<00:00, 305.54it/s]\n",
      "Epoch: 13, Loss: 0.18869, Acc: 0.89019: 100%|██████████| 195/195 [00:02<00:00, 96.91it/s]\n",
      "\tEpoch: 13, Loss: 0.64133, Acc: 0.69156: 100%|██████████| 22/22 [00:00<00:00, 318.82it/s]\n",
      "Epoch: 14, Loss: 0.18372, Acc: 0.89542: 100%|██████████| 195/195 [00:01<00:00, 97.93it/s]\n",
      "\tEpoch: 14, Loss: 0.63679, Acc: 0.69075: 100%|██████████| 22/22 [00:00<00:00, 305.53it/s]\n",
      "Epoch: 15, Loss: 0.18188, Acc: 0.88933: 100%|██████████| 195/195 [00:02<00:00, 96.77it/s] \n",
      "\tEpoch: 15, Loss: 0.64439, Acc: 0.69508: 100%|██████████| 22/22 [00:00<00:00, 318.82it/s]\n",
      "Epoch: 16, Loss: 0.17375, Acc: 0.89218: 100%|██████████| 195/195 [00:02<00:00, 96.29it/s]\n",
      "\tEpoch: 16, Loss: 0.65897, Acc: 0.68655: 100%|██████████| 22/22 [00:00<00:00, 297.28it/s]\n",
      "Epoch: 17, Loss: 0.17093, Acc: 0.89583: 100%|██████████| 195/195 [00:02<00:00, 96.00it/s]\n",
      "\tEpoch: 17, Loss: 0.65072, Acc: 0.69589: 100%|██████████| 22/22 [00:00<00:00, 282.04it/s]\n",
      "Epoch: 18, Loss: 0.16683, Acc: 0.90016: 100%|██████████| 195/195 [00:02<00:00, 95.25it/s]\n",
      "\tEpoch: 18, Loss: 0.65104, Acc: 0.68162: 100%|██████████| 22/22 [00:00<00:00, 241.74it/s]\n",
      "Epoch: 19, Loss: 0.16840, Acc: 0.89679: 100%|██████████| 195/195 [00:02<00:00, 96.83it/s]\n",
      "\tEpoch: 19, Loss: 0.65437, Acc: 0.69873: 100%|██████████| 22/22 [00:00<00:00, 305.54it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "optimizer = torch.optim.Adam(toxicity_classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_loss = 1e9\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_classifier(epoch, toxicity_classifier, optimizer, criterion, train_dataloader, device)\n",
    "    loss = evaluate_classifier(epoch, toxicity_classifier, criterion, val_dataloader, device)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        torch.save(toxicity_classifier.state_dict(), '../models/toxicity_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test to evaluate the performance of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: buttcheeks, Toxicity: 0.9748811721801758\n",
      "Word: university, Toxicity: 0.48354899883270264\n"
     ]
    }
   ],
   "source": [
    "toxic_word = 'buttcheeks'\n",
    "non_toxic_word = 'university'\n",
    "\n",
    "toxicity_classifier.load_state_dict(torch.load('../models/toxicity_classifier.pth'))\n",
    "\n",
    "print(f'Word: {toxic_word}, Toxicity: {get_toxicity(toxic_word, tokenizer, toxicity_classifier)}')\n",
    "print(f'Word: {non_toxic_word}, Toxicity: {get_toxicity(non_toxic_word, tokenizer, toxicity_classifier)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for MaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97006/97006 [07:26<00:00, 217.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(toxic_sentences))):\n",
    "    toxic_sentences[i] = put_mask_with_classifier(toxic_sentences[i], tokenizer, toxicity_classifier)\n",
    "    if '[MASK]' in toxic_sentences[i]:\n",
    "        data.append(toxic_sentences[i])\n",
    "        labels.append(non_toxic_sentences[i])\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e9066af7af47e49203fe0e593d9b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0301ed481640494d814615443068cb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2831, 'learning_rate': 1.819037278320666e-05, 'epoch': 0.09}\n",
      "{'loss': 3.0637, 'learning_rate': 1.638074556641332e-05, 'epoch': 0.18}\n",
      "{'loss': 3.0873, 'learning_rate': 1.4571118349619979e-05, 'epoch': 0.27}\n",
      "{'loss': 2.9544, 'learning_rate': 1.2761491132826638e-05, 'epoch': 0.36}\n",
      "{'loss': 2.9274, 'learning_rate': 1.0951863916033298e-05, 'epoch': 0.45}\n",
      "{'loss': 2.9715, 'learning_rate': 9.142236699239957e-06, 'epoch': 0.54}\n",
      "{'loss': 2.8911, 'learning_rate': 7.332609482446616e-06, 'epoch': 0.63}\n",
      "{'loss': 2.8747, 'learning_rate': 5.5229822656532765e-06, 'epoch': 0.72}\n",
      "{'loss': 2.8676, 'learning_rate': 3.7133550488599353e-06, 'epoch': 0.81}\n",
      "{'loss': 2.8604, 'learning_rate': 1.9037278320665944e-06, 'epoch': 0.9}\n",
      "{'loss': 2.8661, 'learning_rate': 9.410061527325373e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5a650710fb4ec4a84e7025a3ce7b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.749858856201172, 'eval_runtime': 157.18, 'eval_samples_per_second': 62.495, 'eval_steps_per_second': 3.906, 'epoch': 1.0}\n",
      "{'train_runtime': 3724.7467, 'train_samples_per_second': 23.735, 'train_steps_per_second': 1.484, 'train_loss': 2.9680513250107268, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train('maskedlm_with_classifier', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=3,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hurt you and I want to find out who that was.\n",
      "Masked: [MASK] you and i [MASK] to find out who that [MASK]\n",
      "Detoxified: and you and i need to find out who that.\n",
      "\n",
      "Original: I can't believe we haven't fucked for two years, nine months, three weeks and... 69 hours.\n",
      "Masked: i [MASK] believe [MASK] [MASK] fucked [MASK] two years, nine months, [MASK] weeks and... 69 hours.\n",
      "Detoxified: i cannot believe i just fucked up two years, nine months, two weeks and... 69 hours.\n",
      "\n",
      "Original: So forgive me for being a little fidgety, but if it's lunch we're talking, I'm gonna eat a fat pork sandwich, and I'm sure as shit not gonna eat it here.\n",
      "Masked: so forgive me [MASK] [MASK] a little [MASK] but if [MASK] lunch [MASK] talking, i'm [MASK] eat a [MASK] [MASK] sandwich, and i'm [MASK] as [MASK] [MASK] [MASK] eat [MASK] here.\n",
      "Detoxified: so forgive me to just a little. but if your lunch stop talking, i'm to eat a c ing sandwich, and i'm good as good as you eat right here.\n",
      "\n",
      "Original: There is no fucking soy milk!\n",
      "Masked: there is no [MASK] soy [MASK]\n",
      "Detoxified: there is no real soy.\n",
      "\n",
      "Original: What's wrong with people having sex?\n",
      "Masked: what's [MASK] [MASK] people [MASK] sex?\n",
      "Detoxified: what's the about people had sex?\n",
      "\n",
      "Original: What the fuck are you talking about?\n",
      "Masked: what the [MASK] are you talking about?\n",
      "Detoxified: what the world are you talking about?\n",
      "\n",
      "Original: There were fewer horses that needed stealing than men who needed killing, after all.\n",
      "Masked: there were [MASK] horses that needed [MASK] than [MASK] who needed killing, after all.\n",
      "Detoxified: there were better horses that needed fighting than horses who needed killing, after all.\n",
      "\n",
      "Original: Hey, Bob, thanks for giving us a chance to grieve together. Shit!\n",
      "Masked: [MASK] bob, thanks [MASK] giving [MASK] a chance to [MASK] together. [MASK]\n",
      "Detoxified: \" bob, thanks to giving me a chance to get together. \"\n",
      "\n",
      "Original: But my landlord's another fucking story-- Trying to kick me out of my home.\n",
      "Masked: but my landlord's [MASK] [MASK] [MASK] trying to kick me out of my home.\n",
      "Detoxified: but my landlord's a ing ing trying to kick me out of my home.\n",
      "\n",
      "Original: Gryilus, the father of young Phylon. I slashed his face with a whip.\n",
      "Masked: [MASK] the father of young [MASK] i slashed his [MASK] [MASK] a [MASK]\n",
      "Detoxified: when the father of young people i slashed his throat like a.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = BertForMaskedLM.from_pretrained(\"../models/bert_maskedlm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../models/bert_maskedlm\")\n",
    "\n",
    "test_toxic_sentences = pd.read_csv('../data/interim/test.csv')['reference'].to_list()[:10]\n",
    "\n",
    "detoxified = detoxificate_text_with_classifier(test_toxic_sentences, tokenizer, best_model, toxicity_classifier)\n",
    "\n",
    "for sentence, detoxified_sentence in zip(test_toxic_sentences, detoxified):\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Masked: {put_mask_with_classifier(sentence, tokenizer, toxicity_classifier)}')\n",
    "    print(f'Detoxified: {detoxified_sentence}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

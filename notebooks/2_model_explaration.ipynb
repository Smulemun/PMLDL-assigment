{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertGenerationDecoder, BertGenerationEncoder, EncoderDecoderModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f056615e5b449ee9523c0afbb27f500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39851259c65476ca78b86b943197bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertGenerationEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertGenerationEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertGenerationDecoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.11.crossattention.self.key.weight', 'lm_head.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168ec90a5b664f79b0a7fc061765a9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967852b38a4b4da4811be263703a6dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = BertGenerationEncoder.from_pretrained(\"bert-base-uncased\", bos_token_id=101, eos_token_id=102)\n",
    "decoder = BertGenerationDecoder.from_pretrained(\"bert-base-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102)\n",
    "bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input_ids = tokenizer(\"They're all laughing at us, so we'll kick your ass\", add_special_tokens=False, return_tensors='pt').input_ids\n",
    "\n",
    "out = bert2bert.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] indo indo [CLS]zen [CLS]zen [CLS]zen [CLS]zen [CLS]zen [CLS]zen [CLS]zen [CLS]zen [CLS]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2027, 1005, 2128, 2035, 5870, 2012, 2149, 1010, 2061, 2057, 1005,\n",
      "         2222, 5926, 2115, 4632, 1012,  102,    0,    0]]), 'token_type_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "text = \"They're all laughing at us, so we'll kick your ass.\"\n",
    "target = \"they're laughing at us. We'll show you.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding='max_length', max_length=20)\n",
    "encoded_target = tokenizer(target, return_tensors='pt', padding='max_length', max_length=20)\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 768])\n",
      "torch.Size([20])\n",
      "<unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390> <unused390>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(output.to_tuple()[0][0].shape)\n",
    "print(torch.argmax(output.to_tuple()[0][0], dim=1).shape)\n",
    "print(tokenizer.decode(torch.argmax(output.to_tuple()[0][0], dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315f8310b41944829e6ae52d122dc272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30cb4f421024237ba1ea6c7bc93557c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870d5e5f620e41fea4c2e19dd6000821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"They're all laughing at us, so we'll kick your ass.\"\n",
    "target = \"they're laughing at us. We'll show you.\"\n",
    "encoded_input = tokenizer(input, return_tensors='pt', padding='max_length', max_length=20)\n",
    "encoded_target = tokenizer(target, return_tensors='pt', padding='max_length', max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids: Optional[torch.Tensor] = None, attention_mask: Optional[torch.Tensor] = None, token_type_ids: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None, inputs_embeds: Optional[torch.Tensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.Tensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple[torch.Tensor], transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect \n",
    "\n",
    "inspect.signature(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertModel.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment1\\PMLDL-assigment\\notebooks\\2_model_explaration.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment1/PMLDL-assigment/notebooks/2_model_explaration.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input, labels\u001b[39m=\u001b[39;49mencoded_target\u001b[39m.\u001b[39;49mword_ids)\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: BertModel.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1743,  0.1088, -0.2370,  ..., -0.6807,  0.4518, -1.1180],\n",
      "         [ 0.0838, -0.0892, -1.5150,  ..., -1.6850, -0.4736, -1.1984],\n",
      "         [ 0.0075,  0.0226,  0.2443,  ..., -1.0941, -0.1900, -0.5461],\n",
      "         ...,\n",
      "         [ 0.3066, -0.1589, -0.1609,  ..., -0.4859,  1.4342, -1.1660],\n",
      "         [-0.0472, -0.5350, -0.5002,  ...,  0.4122,  0.6989, -0.0691],\n",
      "         [ 0.2613, -0.2891, -0.4838,  ...,  0.5553,  0.6348, -0.2147]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9998, -0.0920, -0.1869,  0.2832,  0.3221,  0.8025,  0.3848, -0.9737,\n",
      "          0.0494, -0.9957,  0.1326, -0.9943, -0.9881, -0.9638,  0.2602,  0.2514,\n",
      "         -0.0776, -0.0446, -0.2372, -0.0084, -0.0528,  0.0090,  0.5601,  0.9323,\n",
      "          0.9960,  0.9965, -0.3003, -0.1516,  0.0070,  0.5191,  0.9905, -0.3604,\n",
      "          0.2999,  0.5248, -0.8439,  0.0772,  0.2423, -0.0721, -0.4314, -0.2391,\n",
      "          0.3504, -0.0799,  0.0676, -0.9998, -0.1554, -0.5372,  0.3502,  0.1115,\n",
      "          0.5475, -0.2636,  0.8799,  0.1200,  0.9998,  0.2252,  0.2843, -0.0828,\n",
      "          0.0784,  0.4868, -0.9999, -0.9995, -0.3822, -0.1429,  0.4776,  0.2387,\n",
      "          0.1631, -0.9959,  0.9824,  0.8480, -0.0303,  0.6604,  0.9858,  1.0000,\n",
      "         -0.8909, -0.8518, -0.6887, -0.0827,  0.0264,  0.9993,  0.0541, -0.2462,\n",
      "         -0.9982, -0.1743,  0.2733,  0.0187,  0.9837, -0.9940,  0.4293, -0.1496,\n",
      "         -0.2055, -0.0806, -0.9924, -0.3938,  0.0428, -0.9993, -0.9187, -0.3762,\n",
      "          0.8764,  0.0514, -0.9998, -0.0381, -0.9680, -0.9939, -0.9979, -0.1294,\n",
      "          0.0506,  0.3054,  0.8894,  0.2486, -0.3970,  0.5571, -0.4250,  0.1503,\n",
      "          0.9998, -0.9941, -0.1865,  0.3815, -0.9235, -0.4014, -0.3344,  0.9774,\n",
      "         -0.9263,  0.0210, -0.9987, -0.9977,  0.0675, -0.2818, -0.1732,  0.3280,\n",
      "         -0.1760, -0.3051,  0.9991,  0.9522,  0.9309,  0.1113,  0.0137, -0.9164,\n",
      "          0.9999, -0.9974, -0.9997,  0.2494,  0.9996,  0.1956, -0.0806,  0.9857,\n",
      "         -0.1335,  0.9999, -0.9563,  0.9548, -0.9948,  0.0653, -0.3717, -0.0634,\n",
      "          0.9266,  0.1304, -0.1332,  0.0939,  0.5952, -0.5443, -0.1645,  0.0117,\n",
      "         -0.0725, -0.1938, -0.0435,  0.0381,  1.0000, -0.5133, -0.1554, -0.2024,\n",
      "          0.9959,  0.9667, -0.2971, -0.8428, -0.9491, -0.3397, -0.1367,  0.3597,\n",
      "         -0.7844, -0.2752,  0.9839, -0.3088, -0.0193,  0.0330,  0.9889, -0.4276,\n",
      "          0.7202, -0.9843, -0.9997, -0.9613, -0.1668,  0.8901,  0.9297, -0.8879,\n",
      "         -0.9879, -0.0877,  0.9966,  0.2717, -0.9770, -0.9954,  0.9624, -0.1392,\n",
      "          0.3674, -0.3110, -0.7368,  0.1569, -0.0171,  0.9971,  0.0056,  0.9999,\n",
      "          0.9991, -0.0135,  0.0044,  0.6769, -0.1738,  0.3936,  0.9898, -0.9993,\n",
      "         -0.7068,  0.9446,  0.8308,  0.2740, -0.9974,  0.3078, -0.6936,  0.4482,\n",
      "         -0.9928, -0.4688, -0.9961,  0.2973, -0.9993,  0.4634,  0.1822,  0.3036,\n",
      "         -0.0276,  0.0441,  0.9976,  0.3859, -0.9869, -0.2715,  0.4541, -0.1585,\n",
      "         -0.9181,  1.0000, -0.7512,  0.9999, -0.2376,  0.3268,  0.9998, -0.0443,\n",
      "          0.9913, -0.3496,  0.1739,  0.1073,  0.0470,  0.1311,  1.0000, -0.0773,\n",
      "         -0.0198, -0.2117, -0.4143,  0.0421, -0.0500, -0.9998,  0.4655,  0.5522,\n",
      "         -0.0979,  0.9979,  0.9862,  0.4832,  0.0398,  0.1749,  0.9940, -0.0313,\n",
      "         -0.9981,  1.0000, -0.1410, -0.9317,  0.9994,  0.1393,  0.3808, -0.1781,\n",
      "         -0.6222, -0.9937,  0.7618,  1.0000,  0.3874, -0.0783,  0.1167, -0.1508,\n",
      "         -0.9990, -0.9947, -0.7091,  0.4087,  0.6155,  0.9952,  0.9981,  0.0014,\n",
      "         -0.0864,  0.9958, -0.1153,  0.9997,  1.0000, -0.9993, -0.5867,  0.9988,\n",
      "         -0.3320,  0.4100,  0.3430,  0.9995, -0.5264, -0.1037,  0.2467, -0.8749,\n",
      "         -0.2123,  0.9919,  0.9566,  0.0076,  0.0615,  0.0547,  0.4206,  0.9994,\n",
      "         -0.0375, -0.3275, -0.4082, -0.9995,  0.0233,  0.2032, -0.8561, -0.4579,\n",
      "         -0.9821,  0.6145, -0.6207,  0.1216,  0.0384,  0.3731, -0.1378, -0.0327,\n",
      "         -0.0778, -0.7877, -0.1168,  0.0336,  0.9966,  0.2471,  0.0489,  0.9974,\n",
      "         -0.9232,  0.9119,  0.8539,  0.9881,  0.1541,  0.4566,  0.0120, -0.0944,\n",
      "          0.5802,  0.0958,  0.8230,  0.1987, -0.2115, -0.0850, -0.2008, -0.0814,\n",
      "         -0.9999, -0.5730, -0.0151, -0.3149,  0.2481, -0.9909, -0.6062, -0.1870,\n",
      "         -0.2731, -0.2353,  0.3914,  0.9992, -0.3020,  0.3439,  0.9252, -0.9039,\n",
      "          0.0026, -0.4355,  0.0117, -0.0247,  0.8244,  0.2435, -0.9282, -0.2208,\n",
      "          0.9975, -0.3774,  0.5203, -0.0963, -0.7259, -0.3037, -0.1482,  0.9194,\n",
      "         -1.0000, -0.1715, -0.1464, -0.2122, -0.0441, -0.4386,  0.9957, -0.0914,\n",
      "          0.5824, -0.9906, -0.8884, -0.0355,  0.9954, -0.8165,  0.3536, -0.0081,\n",
      "          0.0380, -0.9970,  0.9837,  0.9872, -0.9647, -0.3975, -0.9968,  0.2523,\n",
      "          0.1385,  0.0280,  0.1354, -1.0000, -0.1736,  1.0000, -0.2404,  0.9536,\n",
      "          0.3185,  0.5515, -0.0725, -0.2473, -0.2741,  0.3110, -0.0711, -0.4635,\n",
      "          0.9998, -1.0000, -0.6727,  0.0211, -0.9551, -0.8413, -0.2098,  0.2478,\n",
      "          0.0534, -0.9999, -0.9987, -0.2294, -1.0000, -0.9846,  0.8406,  0.0499,\n",
      "         -0.0809,  0.0515,  0.3187,  0.3293, -0.2841,  0.9720, -0.0538,  0.6639,\n",
      "          0.2220, -0.0970, -0.2070,  0.9962,  0.9970,  0.2091, -1.0000,  0.2239,\n",
      "          0.9855,  0.9999,  0.9032,  0.6709, -0.9412, -0.0552,  0.3638, -0.9294,\n",
      "          0.6151,  0.0618, -0.0585, -1.0000, -0.9970,  0.3227, -0.9803,  0.9999,\n",
      "         -0.2750,  0.3173,  0.1606, -0.1393,  0.2425, -0.9992, -0.9988, -0.2807,\n",
      "          0.6344,  0.1120, -0.0646, -1.0000, -0.9955, -0.6392, -0.2898,  0.0623,\n",
      "         -0.1255,  0.2196, -0.9988,  0.8716, -0.9901,  0.0311, -0.0726, -0.0322,\n",
      "         -0.0784, -0.7086,  0.9984, -0.8842, -0.1962,  0.3725, -0.1642, -0.8667]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

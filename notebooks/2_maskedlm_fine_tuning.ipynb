{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Hypothesis: Mask toxic words and use MaskedLM to find appropriate alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2656fbae410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import put_mask\n",
    "from src.models.predict import detoxificate_text\n",
    "from src.models.train import train\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading bert-base-uncased model for MaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97006/97006 [00:50<00:00, 1920.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(toxic_sentences))):\n",
    "    toxic_sentences[i] = put_mask(toxic_sentences[i], toxic_words)\n",
    "    if '[MASK]' in toxic_sentences[i]:\n",
    "        data.append(toxic_sentences[i])\n",
    "        labels.append(non_toxic_sentences[i])\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e1c5fa016f4740b8b7d959c342d77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/86397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using the Hugging Face trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7326bbfa37514e56aa4deb202f1cc8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.476, 'learning_rate': 1.9314128943758576e-05, 'epoch': 0.1}\n",
      "{'loss': 2.2942, 'learning_rate': 1.862825788751715e-05, 'epoch': 0.21}\n",
      "{'loss': 2.2701, 'learning_rate': 1.7942386831275723e-05, 'epoch': 0.31}\n",
      "{'loss': 2.245, 'learning_rate': 1.7256515775034294e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1805, 'learning_rate': 1.6570644718792868e-05, 'epoch': 0.51}\n",
      "{'loss': 2.1607, 'learning_rate': 1.588477366255144e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2014, 'learning_rate': 1.5198902606310016e-05, 'epoch': 0.72}\n",
      "{'loss': 2.1563, 'learning_rate': 1.4513031550068588e-05, 'epoch': 0.82}\n",
      "{'loss': 2.1493, 'learning_rate': 1.3827160493827162e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa6cee8615f467b8ed43042b040c4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9825509786605835, 'eval_runtime': 137.9325, 'eval_samples_per_second': 62.639, 'eval_steps_per_second': 3.915, 'epoch': 1.0}\n",
      "{'loss': 2.0801, 'learning_rate': 1.3141289437585736e-05, 'epoch': 1.03}\n",
      "{'loss': 2.0414, 'learning_rate': 1.2455418381344308e-05, 'epoch': 1.13}\n",
      "{'loss': 2.019, 'learning_rate': 1.1769547325102882e-05, 'epoch': 1.23}\n",
      "{'loss': 2.0071, 'learning_rate': 1.1083676268861454e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0173, 'learning_rate': 1.039780521262003e-05, 'epoch': 1.44}\n",
      "{'loss': 1.9853, 'learning_rate': 9.711934156378602e-06, 'epoch': 1.54}\n",
      "{'loss': 1.9916, 'learning_rate': 9.026063100137174e-06, 'epoch': 1.65}\n",
      "{'loss': 1.9981, 'learning_rate': 8.340192043895748e-06, 'epoch': 1.75}\n",
      "{'loss': 1.9776, 'learning_rate': 7.654320987654322e-06, 'epoch': 1.85}\n",
      "{'loss': 1.9767, 'learning_rate': 6.968449931412895e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3927edd10a459880b56804676268ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8876408338546753, 'eval_runtime': 137.6567, 'eval_samples_per_second': 62.765, 'eval_steps_per_second': 3.923, 'epoch': 2.0}\n",
      "{'loss': 1.9605, 'learning_rate': 6.282578875171468e-06, 'epoch': 2.06}\n",
      "{'loss': 1.9216, 'learning_rate': 5.596707818930042e-06, 'epoch': 2.16}\n",
      "{'loss': 1.9267, 'learning_rate': 4.910836762688615e-06, 'epoch': 2.26}\n",
      "{'loss': 1.8795, 'learning_rate': 4.224965706447189e-06, 'epoch': 2.37}\n",
      "{'loss': 1.9247, 'learning_rate': 3.5390946502057617e-06, 'epoch': 2.47}\n",
      "{'loss': 1.9298, 'learning_rate': 2.8532235939643347e-06, 'epoch': 2.57}\n",
      "{'loss': 1.9279, 'learning_rate': 2.167352537722908e-06, 'epoch': 2.67}\n",
      "{'loss': 1.9018, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
      "{'loss': 1.9098, 'learning_rate': 7.956104252400549e-07, 'epoch': 2.88}\n",
      "{'loss': 1.8872, 'learning_rate': 1.0973936899862826e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd4a6d4d27e49b9871b227d1e33585f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8983267545700073, 'eval_runtime': 137.3143, 'eval_samples_per_second': 62.921, 'eval_steps_per_second': 3.933, 'epoch': 3.0}\n",
      "{'train_runtime': 9772.7218, 'train_samples_per_second': 23.87, 'train_steps_per_second': 1.492, 'train_loss': 2.047203666781202, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "train('maskedlm', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=3,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hurt you and I want to find out who that was.\n",
      "Masked: [MASK] you and i want to find out who that was.\n",
      "Detoxified: both you and i want to find out who that was.\n",
      "\n",
      "Original: I can't believe we haven't fucked for two years, nine months, three weeks and... 69 hours.\n",
      "Masked: i can't [MASK]ve we haven't [MASK]ed for two years, nine months, three weeks and... 69 hours.\n",
      "Detoxified: i can't've we haven't been ed for two years, nine months, three weeks and... 69 hours.\n",
      "\n",
      "Original: So forgive me for being a little fidgety, but if it's lunch we're talking, I'm gonna eat a fat pork sandwich, and I'm sure as shit not gonna eat it here.\n",
      "Masked: so forgive me for being a little [MASK]y, but if it's lunch we're talking, i'm gonna eat a [MASK] pork sandwich, and i'm sure as [MASK] not gonna eat it here.\n",
      "Detoxified: so forgive me for being a little co y, but if it's lunch we're talking, i'm gonna eat a roast pork sandwich, and i'm sure as well not gonna eat it here.\n",
      "\n",
      "Original: There is no fucking soy milk!\n",
      "Masked: there is no [MASK]ing soy milk!\n",
      "Detoxified: there is no - ing soy milk!\n",
      "\n",
      "Original: What's wrong with people having sex?\n",
      "Masked: what's [MASK] with people having [MASK]?\n",
      "Detoxified: what's it with people having babies?\n",
      "\n",
      "Original: What the fuck are you talking about?\n",
      "Masked: what the [MASK] are you talking about?\n",
      "Detoxified: what the world are you talking about?\n",
      "\n",
      "Original: There were fewer horses that needed stealing than men who needed killing, after all.\n",
      "Masked: there were fewer horses that needed [MASK] than men who needed [MASK]ing, after all.\n",
      "Detoxified: there were fewer horses that needed training than men who needed an ing, after all.\n",
      "\n",
      "Original: Hey, Bob, thanks for giving us a chance to grieve together. Shit!\n",
      "Masked: hey, bob, thanks for giving us a chance to [MASK] together. [MASK]!\n",
      "Detoxified: hey, bob, thanks for giving us a chance to get together. oh!\n",
      "\n",
      "Original: But my landlord's another fucking story-- Trying to kick me out of my home.\n",
      "Masked: but my landlord's another [MASK]ing story-- trying to kick me out of my home.\n",
      "Detoxified: but my landlord's another - ing story - - trying to kick me out of my home.\n",
      "\n",
      "Original: Gryilus, the father of young Phylon. I slashed his face with a whip.\n",
      "Masked: gryilus, the [MASK]her of young phylon. i slashed his face with a whip.\n",
      "Detoxified: gryilus, the little her of young phylon. i slashed his face with a whip.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = BertForMaskedLM.from_pretrained(\"../models/bert_maskedlm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../models/bert_maskedlm\")\n",
    "\n",
    "test_toxic_sentences = pd.read_csv('../data/interim/test.csv')['reference'].to_list()[:10]\n",
    "\n",
    "detoxified = detoxificate_text(test_toxic_sentences, toxic_words, tokenizer, best_model)\n",
    "\n",
    "for sentence, detoxified_sentence in zip(test_toxic_sentences, detoxified):\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Masked: {put_mask(sentence, toxic_words)}')\n",
    "    print(f'Detoxified: {detoxified_sentence}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

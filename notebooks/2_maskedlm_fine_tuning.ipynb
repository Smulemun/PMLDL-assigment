{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e8eec4e410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocess import put_mask\n",
    "from src.models.predict import detoxificate_text\n",
    "from src.models.train import train\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101535/101535 [01:02<00:00, 1613.41it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/train.csv')\n",
    "toxic_sentences = df['reference'].tolist()\n",
    "non_toxic_sentences = df['translation'].tolist()\n",
    "toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(toxic_sentences))):\n",
    "    toxic_sentences[i] = put_mask(toxic_sentences[i], toxic_words)\n",
    "    if '[MASK]' in toxic_sentences[i]:\n",
    "        data.append(toxic_sentences[i])\n",
    "        labels.append(non_toxic_sentences[i])\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2901ba580bbe418f85c18eb18b6537c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    inputs = [ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3a816cd25348b29a5a45f24746640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6986, 'learning_rate': 1.8022933965994466e-05, 'epoch': 0.1}\n",
      "{'loss': 2.5543, 'learning_rate': 1.604586793198893e-05, 'epoch': 0.2}\n",
      "{'loss': 2.5081, 'learning_rate': 1.4068801897983393e-05, 'epoch': 0.3}\n",
      "{'loss': 2.4367, 'learning_rate': 1.2091735863977859e-05, 'epoch': 0.4}\n",
      "{'loss': 2.4347, 'learning_rate': 1.0114669829972321e-05, 'epoch': 0.49}\n",
      "{'loss': 2.4175, 'learning_rate': 8.137603795966786e-06, 'epoch': 0.59}\n",
      "{'loss': 2.4153, 'learning_rate': 6.160537761961251e-06, 'epoch': 0.69}\n",
      "{'loss': 2.3735, 'learning_rate': 4.183471727955714e-06, 'epoch': 0.79}\n",
      "{'loss': 2.4114, 'learning_rate': 2.2064056939501782e-06, 'epoch': 0.89}\n",
      "{'loss': 2.3053, 'learning_rate': 2.2933965994464219e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a059de58288f4051938a00b533b52205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.310804843902588, 'eval_runtime': 144.0731, 'eval_samples_per_second': 62.413, 'eval_steps_per_second': 3.901, 'epoch': 1.0}\n",
      "{'train_runtime': 3410.0589, 'train_samples_per_second': 23.73, 'train_steps_per_second': 1.483, 'train_loss': 2.454359945264811, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train('maskedlm', \n",
    "      model, \n",
    "      tokenizer, \n",
    "      train_dataset, \n",
    "      val_dataset, \n",
    "      data_collator,\n",
    "      batch_size=16, \n",
    "      epochs=1,\n",
    "      seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Suddenly, to the delight and outrage of the congregation, a raucous saxophone broke the solemnity, and a jazz rendering of \"Fools Rush In\" was blaring over the loudspeakers.\n",
      "Masked: suddenly, to the delight and out[MASK] of the congregation, a raucous s[MASK]ophone [MASK] the [MASK]ity, and a jazz rendering of \"[MASK]s rush in\" was blaring over the [MASK]speakers.\n",
      "Detoxified: suddenly, to the delight and outflow of the congregation, a raucous s was ophoned the - ity, and a jazz rendering of \"'s rush in \" was blaring over the stage speakers.\n",
      "\n",
      "Original: This place is such a dump.\n",
      "Masked: this place is such a [MASK].\n",
      "Detoxified: this place is such a place.\n",
      "\n",
      "Original: Doesn't mean a damn thing!\n",
      "Masked: doesn't mean a [MASK] thing!\n",
      "Detoxified: doesn't mean a first thing!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "best_model = BertForMaskedLM.from_pretrained(\"../models/bert_maskedlm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../models/bert_maskedlm\")\n",
    "\n",
    "random_toxic_sentences = random.sample(df['reference'].tolist(), 3)\n",
    "\n",
    "for sentence in random_toxic_sentences:\n",
    "    print(f'Original: {sentence}')\n",
    "    print(f'Masked: {put_mask(sentence, toxic_words)}')\n",
    "    print(f'Detoxified: {detoxificate_text(sentence, toxic_words, tokenizer, best_model)}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/filtered.tsv', sep='\\t')\n",
    "sents = df[(df['similarity'] < 0.7) & (df['ref_tox'] > df['trn_tox'])]\n",
    "sents = sents[['reference', 'translation']]\n",
    "toxic_sentences = sents['reference'].tolist()\n",
    "non_toxic_sentences = sents['translation'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9d3304fdd848648549297ae971866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb10687a951484f857edd794e2fc1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f066a703164443eadab0582e42d2748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720bb6ad62a346ef826f106f44ea74f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3640bf61cf0b4bbb8076fb4b1b5ea31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295bb5dbe49642529b90abdb9d7ee178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "# encoder = BertGenerationEncoder.from_pretrained(model_name, bos_token_id=101, eos_token_id=102)\n",
    "# decoder = BertGenerationDecoder.from_pretrained(model_name, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102)\n",
    "# bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "# bert2bert.config.decoder_start_token_id = 101\n",
    "# bert2bert.config.pad_token_id = 0\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=bert2bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "prefix = 'Detoxify text: '\n",
    "\n",
    "def preprocess_text(examples):\n",
    "    inputs = [prefix + ex for ex in examples['text']]\n",
    "    target = [ex for ex in examples['labels']]\n",
    "\n",
    "    batch = tokenizer(inputs, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt')\n",
    "    batch[\"labels\"] = tokenizer(target, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a57006b69fa47ecb769cf7fd11cb2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\"text\": toxic_sentences, \"labels\": non_toxic_sentences})\n",
    "dataset = dataset.map(preprocess_text, batched=True)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxificate_text(text, model, tokenizer):\n",
    "    text = prefix + text\n",
    "    test_input = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = test_input.input_ids\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, max_length=MAX_LEN)\n",
    "    non_toxic_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return non_toxic_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You gotta be shitting me.\n",
      "tensor([[    0, 10747,     7,    15,     1]])\n",
      "<pad> False</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"You gotta be shitting me.\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "print(detoxificate_text(text, model.to(\"cpu\"), tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42ac0c81a64b9da16a39f22a03e97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2574, 'learning_rate': 1.9416433239962655e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2506, 'learning_rate': 1.8832866479925304e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2499, 'learning_rate': 1.8249299719887958e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2437, 'learning_rate': 1.7665732959850607e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2455, 'learning_rate': 1.708216619981326e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2495, 'learning_rate': 1.649859943977591e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2455, 'learning_rate': 1.5915032679738563e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2471, 'learning_rate': 1.5331465919701216e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2423, 'learning_rate': 1.4747899159663868e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2438, 'learning_rate': 1.416433239962652e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2413, 'learning_rate': 1.358076563958917e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e37878facb4d1f9f31008e1122f085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2224476933479309, 'eval_runtime': 83.8485, 'eval_samples_per_second': 121.099, 'eval_steps_per_second': 7.573, 'epoch': 1.0}\n",
      "{'loss': 0.2396, 'learning_rate': 1.2997198879551822e-05, 'epoch': 1.05}\n",
      "{'loss': 0.2406, 'learning_rate': 1.2413632119514474e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2403, 'learning_rate': 1.1830065359477125e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2376, 'learning_rate': 1.1246498599439776e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2382, 'learning_rate': 1.0662931839402428e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2425, 'learning_rate': 1.007936507936508e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2376, 'learning_rate': 9.49579831932773e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2413, 'learning_rate': 8.912231559290384e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2389, 'learning_rate': 8.328664799253035e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2362, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2381, 'learning_rate': 7.161531279178339e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efcf1cdb5434703b1f39a98be46365c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21875521540641785, 'eval_runtime': 83.8575, 'eval_samples_per_second': 121.086, 'eval_steps_per_second': 7.572, 'epoch': 2.0}\n",
      "{'loss': 0.2409, 'learning_rate': 6.5779645191409904e-06, 'epoch': 2.01}\n",
      "{'loss': 0.2385, 'learning_rate': 5.994397759103642e-06, 'epoch': 2.1}\n",
      "{'loss': 0.2358, 'learning_rate': 5.410830999066293e-06, 'epoch': 2.19}\n",
      "{'loss': 0.2354, 'learning_rate': 4.8272642390289456e-06, 'epoch': 2.28}\n",
      "{'loss': 0.237, 'learning_rate': 4.243697478991597e-06, 'epoch': 2.36}\n",
      "{'loss': 0.2349, 'learning_rate': 3.6601307189542484e-06, 'epoch': 2.45}\n",
      "{'loss': 0.238, 'learning_rate': 3.0765639589169007e-06, 'epoch': 2.54}\n",
      "{'loss': 0.2359, 'learning_rate': 2.492997198879552e-06, 'epoch': 2.63}\n",
      "{'loss': 0.2343, 'learning_rate': 1.9094304388422036e-06, 'epoch': 2.71}\n",
      "{'loss': 0.2396, 'learning_rate': 1.3258636788048554e-06, 'epoch': 2.8}\n",
      "{'loss': 0.235, 'learning_rate': 7.42296918767507e-07, 'epoch': 2.89}\n",
      "{'loss': 0.2361, 'learning_rate': 1.5873015873015874e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2982f4aab524a5cb36746e13afc5403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21781419217586517, 'eval_runtime': 84.768, 'eval_samples_per_second': 119.786, 'eval_steps_per_second': 7.491, 'epoch': 3.0}\n",
      "{'train_runtime': 7031.5609, 'train_samples_per_second': 38.988, 'train_steps_per_second': 2.437, 'train_loss': 0.24080748106139938, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17136, training_loss=0.24080748106139938, metrics={'train_runtime': 7031.5609, 'train_samples_per_second': 38.988, 'train_steps_per_second': 2.437, 'train_loss': 0.24080748106139938, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"../models/\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/dotixificator\\\\tokenizer_config.json',\n",
       " '../models/dotixificator\\\\special_tokens_map.json',\n",
       " '../models/dotixificator\\\\spiece.model',\n",
       " '../models/dotixificator\\\\added_tokens.json',\n",
       " '../models/dotixificator\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../models/dotixificator\")\n",
    "tokenizer.save_pretrained(\"../models/dotixificator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you wanna be there, you're gonna have to go my fucking way.\n",
      "if you want to be there, you'll have to go.\n",
      "\n",
      "Come on, get up. Don't be silly.\n",
      "come on, get up, don't be silly.\n",
      "\n",
      "I live in hope that it may pass, but that wretched man persists in writing to her.\n",
      "I live in hope that it will pass, but the wretched man is still writing to her.\n",
      "\n",
      "I'll pray for your soul, sue your ass into the dirt, and wait for the day I can beat it bloody.\n",
      "I'll pray for your soul, I'll sue you in the dirt, and I'll wait for the day I'll beat it.\n",
      "\n",
      "Fuck. No one wants to see Proctor go down more than her.\n",
      "no one wants to see Proctor go down more than her.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_toxic_sentences = random.sample(toxic_sentences, 5)\n",
    "\n",
    "for text in random_toxic_sentences:\n",
    "    print(text)\n",
    "    print(detoxificate_text(text, model.to(\"cpu\"), tokenizer))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

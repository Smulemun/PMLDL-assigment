{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(sentence1, sentence2):\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    senteces = [sentence1, sentence2]\n",
    "    sentence_embeddings = model.encode(senteces)\n",
    "    return cos_sim(sentence_embeddings[0], sentence_embeddings[1]).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9554343223571777\n"
     ]
    }
   ],
   "source": [
    "text1 = \"come on, Cal, leave that shit alone.\"\n",
    "text2 = \"come on, Cal, leave it alone.\"\n",
    "\n",
    "print(semantic_similarity(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_accuracy(text):\n",
    "\n",
    "    # 1 - non-toxic\n",
    "    # 0 - toxic\n",
    "\n",
    "    model_name = 'SkolkovoInstitute/roberta_toxicity_classifier'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(text, return_tensors='pt', padding=True)\n",
    "        logits = model(**encoded).logits\n",
    "        result = torch.softmax(logits, dim=1)[:, 0].item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99996\n"
     ]
    }
   ],
   "source": [
    "text1 = \"If you wanna be there, you're gonna have to go my fucking way.\"\n",
    "text2 = \"if you want to be there, you'll have to go.\"\n",
    "\n",
    "print(f'{style_accuracy(text1):.5f}')\n",
    "print(f'{style_accuracy(text2):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency(text):\n",
    "\n",
    "    # 1 - fluent\n",
    "    # 0 - non-fluent\n",
    "\n",
    "    model_name = \"cointegrated/roberta-large-cola-krishna2020\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(text, return_tensors='pt', padding=True)\n",
    "        logits = model(**encoded).logits\n",
    "        result = torch.softmax(logits, dim=1)[:, 0].item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99158\n",
      "0.46748\n"
     ]
    }
   ],
   "source": [
    "text1 = \"If you wanna be there, you're gonna have to go my fucking way.\"\n",
    "text2 = \"if you wants to be there, you is have to go.\"\n",
    "\n",
    "print(f'{fluency(text1):.5f}')\n",
    "print(f'{fluency(text2):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def j_metric(predictions, targets):\n",
    "    j = 0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        j += semantic_similarity(prediction, target) * style_accuracy(prediction) * fluency(prediction)\n",
    "    return j / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.821343247901035e-05\n",
      "0.7695345282554626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7031139072496444e-05\n",
      "0.9900582432746887\n"
     ]
    }
   ],
   "source": [
    "pred = \"if you want to be there, you'll have to go.\"\n",
    "target = \"if you want to come with me, he'll have to be on my own.\"\n",
    "\n",
    "print(j_metric([pred], [target]))\n",
    "print(semantic_similarity(pred, target))\n",
    "print(style_accuracy(pred))\n",
    "print(fluency(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

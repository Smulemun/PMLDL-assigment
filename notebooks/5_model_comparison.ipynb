{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models using metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f522e3a450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.models.predict import detoxificate_text, detoxificate_text_with_classifier, detoxificate_style_transfer\n",
    "from src.models.metrics import semantic_similarity, style_accuracy, fluency, j_metric\n",
    "from src.models.classifier import ToxicWordsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all necessery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "df = pd.read_csv('../data/interim/test.csv')\n",
    "data = df['reference'].tolist()[:128]\n",
    "labels = df['translation'].tolist()[:128]\n",
    "\n",
    "data = [data[i:i + BATCH_SIZE] for i in range(0, len(data), BATCH_SIZE)]\n",
    "labels = [labels[i:i + BATCH_SIZE] for i in range(0, len(labels), BATCH_SIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_maskedlm = BertForMaskedLM.from_pretrained('../models/bert_maskedlm')\n",
    "tokenizer_maskedlm = BertTokenizer.from_pretrained('../models/bert_maskedlm')\n",
    "\n",
    "classifier = ToxicWordsClassifier(vocab_size=tokenizer_maskedlm.vocab_size, embedding_dim=512, dropout=0.3)\n",
    "classifier.load_state_dict(torch.load('../models/toxicity_classifier.pth'))\n",
    "\n",
    "model_seq2seq = AutoModelForSeq2SeqLM.from_pretrained('../models/detoxificator')\n",
    "tokenizer_seq2seq = AutoTokenizer.from_pretrained('../models/detoxificator')\n",
    "\n",
    "toxic_words = open('../data/interim/toxic_words.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the first hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:19<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "acc = []\n",
    "flnc = []\n",
    "j_score = []\n",
    "\n",
    "for text, label in tqdm(zip(data, labels), total=len(data)):\n",
    "    pred = detoxificate_text(text, toxic_words, tokenizer_maskedlm, model_maskedlm, 'cuda')\n",
    "    sim.append(semantic_similarity(pred, label))\n",
    "    acc.append(style_accuracy(pred))\n",
    "    flnc.append(fluency(pred))\n",
    "    j_score.append(j_metric(sim[-1], acc[-1], flnc[-1]))\n",
    "\n",
    "maskedlm = {}\n",
    "maskedlm['SIM'] = np.mean(sim)\n",
    "maskedlm['ACC'] = np.mean(acc)\n",
    "maskedlm['FLNC'] = np.mean(flnc)\n",
    "maskedlm['J'] = np.mean(j_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIM': 0.8020419, 'ACC': 0.7826125, 'FLNC': 0.7395862, 'J': 0.46810427}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskedlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the second hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "acc = []\n",
    "flnc = []\n",
    "j_score = []\n",
    "\n",
    "for text, label in tqdm(zip(data, labels), total=len(data)):\n",
    "    pred = detoxificate_text_with_classifier(text, tokenizer_maskedlm, model_maskedlm, classifier, 'cuda')\n",
    "    sim.append(semantic_similarity(pred, label))\n",
    "    acc.append(style_accuracy(pred))\n",
    "    flnc.append(fluency(pred))\n",
    "    j_score.append(j_metric(sim[-1], acc[-1], flnc[-1]))\n",
    "\n",
    "maskedlm_with_classifier = {}\n",
    "maskedlm_with_classifier['SIM'] = np.mean(sim)\n",
    "maskedlm_with_classifier['ACC'] = np.mean(acc)\n",
    "maskedlm_with_classifier['FLNC'] = np.mean(flnc)\n",
    "maskedlm_with_classifier['J'] = np.mean(j_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIM': 0.73259705, 'ACC': 0.71623766, 'FLNC': 0.52562416, 'J': 0.2807945}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskedlm_with_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the third hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "acc = []\n",
    "flnc = []\n",
    "j_score = []\n",
    "\n",
    "for text, label in tqdm(zip(data, labels), total=len(data)):\n",
    "    pred = detoxificate_style_transfer(text, model_seq2seq, tokenizer_seq2seq, 'cuda')\n",
    "    sim.append(semantic_similarity(pred, label))\n",
    "    acc.append(style_accuracy(pred))\n",
    "    flnc.append(fluency(pred))\n",
    "    j_score.append(j_metric(sim[-1], acc[-1], flnc[-1]))\n",
    "\n",
    "style_transfer = {}\n",
    "style_transfer['SIM'] = np.mean(sim)\n",
    "style_transfer['ACC'] = np.mean(acc)\n",
    "style_transfer['FLNC'] = np.mean(flnc)\n",
    "style_transfer['J'] = np.mean(j_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIM': 0.8737449, 'ACC': 0.63149583, 'FLNC': 0.88829887, 'J': 0.48831818}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_transfer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
